{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae6fe301-4dad-4e24-a3ea-96cdd3948df0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[3]: True"
     ]
    }
   ],
   "source": [
    "dbutils.fs.cp(\n",
    "  'dbfs:/FileStore/Spark_The_Definitive_Guide_master.zip', \n",
    "  'file:/tmp/Spark_The_Definitive_Guide_master.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbf22dbf-5cc2-47a8-8144-0bb9a415efc5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /tmp/Spark_The_Definitive_Guide_master.zip\n4ba5601eb9b9aed1d01ab79775e3af228216ff6f\n   creating: /tmp/Spark-The-Definitive-Guide-master/\n  inflating: /tmp/Spark-The-Definitive-Guide-master/.gitignore  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/README.md  \n   creating: /tmp/Spark-The-Definitive-Guide-master/code/\n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/A_Gentle_Introduction_to_Spark-Chapter_1_Defining_Spark.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/A_Gentle_Introduction_to_Spark-Chapter_2_A_Gentle_Introduction_to_Spark.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/A_Gentle_Introduction_to_Spark-Chapter_2_A_Gentle_Introduction_to_Spark.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/A_Gentle_Introduction_to_Spark-Chapter_3_A_Tour_of_Sparks_Toolset.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/A_Gentle_Introduction_to_Spark-Chapter_3_A_Tour_of_Sparks_Toolset.r  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/A_Gentle_Introduction_to_Spark-Chapter_3_A_Tour_of_Sparks_Toolset.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/A_Gentle_Introduction_to_Spark_Chapter_2_A_Gentle_Introduction_to_Spark.java  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Advanced_Analytics_and_Machine_Learning-Chapter_24_Advanced_Analytics_and_Machine_Learning.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Advanced_Analytics_and_Machine_Learning-Chapter_24_Advanced_Analytics_and_Machine_Learning.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Advanced_Analytics_and_Machine_Learning-Chapter_25_Preprocessing_and_Feature_Engineering.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Advanced_Analytics_and_Machine_Learning-Chapter_25_Preprocessing_and_Feature_Engineering.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Advanced_Analytics_and_Machine_Learning-Chapter_26_Classification.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Advanced_Analytics_and_Machine_Learning-Chapter_26_Classification.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Advanced_Analytics_and_Machine_Learning-Chapter_27_Regression.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Advanced_Analytics_and_Machine_Learning-Chapter_27_Regression.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Advanced_Analytics_and_Machine_Learning-Chapter_28_Recommendation.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Advanced_Analytics_and_Machine_Learning-Chapter_28_Recommendation.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Advanced_Analytics_and_Machine_Learning-Chapter_29_Unsupervised_Learning.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Advanced_Analytics_and_Machine_Learning-Chapter_29_Unsupervised_Learning.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Advanced_Analytics_and_Machine_Learning-Chapter_30_Graph_Analysis.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Advanced_Analytics_and_Machine_Learning-Chapter_30_Graph_Analysis.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Advanced_Analytics_and_Machine_Learning-Chapter_31_Deep_Learning.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Advanced_Analytics_and_Machine_Learning_Chapter_24_Advanced_Analytics_and_Machine_Learning.java  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Ecosystem-Chapter_32_Language_Specifics.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Ecosystem-Chapter_32_Language_Specifics.r  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Ecosystem-Chapter_33_Ecosystem_and_Community.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Low_Level_APIs-Chapter_12_RDD_Basics.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Low_Level_APIs-Chapter_12_RDD_Basics.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Low_Level_APIs-Chapter_13_Advanced_RDDs.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Low_Level_APIs-Chapter_13_Advanced_RDDs.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Low_Level_APIs-Chapter_14_Distributed_Variables.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Low_Level_APIs-Chapter_14_Distributed_Variables.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Production_Applications-Chapter_15_How_Spark_Runs_on_a_Cluster.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Production_Applications-Chapter_15_How_Spark_Runs_on_a_Cluster.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Production_Applications-Chapter_16_Spark_Applications.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Production_Applications-Chapter_16_Spark_Applications.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Production_Applications-Chapter_17_Deploying_Spark.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Production_Applications-Chapter_18_Monitoring_and_Debugging.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Production_Applications-Chapter_18_Monitoring_and_Debugging.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Production_Applications-Chapter_19_Performance_Tuning.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Streaming-Chapter_21_Structured_Streaming_Basics.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Streaming-Chapter_21_Structured_Streaming_Basics.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Streaming-Chapter_21_Structured_Streaming_Basics.sql  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Streaming-Chapter_22_Event-Time_and_Stateful_Processing.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Streaming-Chapter_22_Event-Time_and_Stateful_Processing.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Streaming-Chapter_22_Event-Time_and_Stateful_Processing.sql  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Streaming-Chapter_23_Structured_Streaming_in_Production.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Streaming-Chapter_23_Structured_Streaming_in_Production.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Structured_APIs-Chapter_10_Spark_SQL.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Structured_APIs-Chapter_10_Spark_SQL.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Structured_APIs-Chapter_10_Spark_SQL.sql  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Structured_APIs-Chapter_11_Datasets.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Structured_APIs-Chapter_4_Structured_API_Overview.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Structured_APIs-Chapter_4_Structured_API_Overview.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Structured_APIs-Chapter_5_Basic_Structured_Operations.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Structured_APIs-Chapter_5_Basic_Structured_Operations.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Structured_APIs-Chapter_5_Basic_Structured_Operations.sql  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Structured_APIs-Chapter_6_Working_with_Different_Types_of_Data.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Structured_APIs-Chapter_6_Working_with_Different_Types_of_Data.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Structured_APIs-Chapter_6_Working_with_Different_Types_of_Data.sql  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Structured_APIs-Chapter_7_Aggregations.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Structured_APIs-Chapter_7_Aggregations.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Structured_APIs-Chapter_7_Aggregations.sql  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Structured_APIs-Chapter_8_Joins.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Structured_APIs-Chapter_8_Joins.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Structured_APIs-Chapter_8_Joins.sql  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Structured_APIs-Chapter_9_Data_Sources.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Structured_APIs-Chapter_9_Data_Sources.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Structured_APIs_Chapter_4_Structured_API_Overview.java  \n   creating: /tmp/Spark-The-Definitive-Guide-master/data/\n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/README.md  \n   creating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/\n extracting: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/_SUCCESS  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/_committed_730451297822678341  \n extracting: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/_started_730451297822678341  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00000-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00001-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00002-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00003-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00004-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00005-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00006-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00007-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00008-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00009-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00010-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00011-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00012-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00013-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00014-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00015-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00016-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00017-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00018-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00019-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00020-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00021-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00022-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00023-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00024-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00025-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00026-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00027-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00028-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00029-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00030-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00031-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00032-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00033-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00034-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00035-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00036-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00037-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00038-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00039-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00040-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00041-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00042-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00043-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00044-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00045-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00046-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00047-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00048-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00049-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00050-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00051-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00052-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00053-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00054-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00055-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00056-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00057-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00058-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00059-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00060-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00061-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00062-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00063-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00064-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00065-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00066-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00067-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00068-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00069-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00070-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00071-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00072-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00073-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00074-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00075-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00076-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00077-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00078-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00079-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n   creating: /tmp/Spark-The-Definitive-Guide-master/data/bike-data/\n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/bike-data/201508_station_data.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/bike-data/201508_trip_data.csv  \n   creating: /tmp/Spark-The-Definitive-Guide-master/data/binary-classification/\n extracting: /tmp/Spark-The-Definitive-Guide-master/data/binary-classification/_SUCCESS  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/binary-classification/part-r-00000-e02e56d5-d522-4b93-a7f2-f2dc1b2fdba9.gz.parquet  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/binary-classification/part-r-00001-e02e56d5-d522-4b93-a7f2-f2dc1b2fdba9.gz.parquet  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/binary-classification/part-r-00002-e02e56d5-d522-4b93-a7f2-f2dc1b2fdba9.gz.parquet  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/binary-classification/part-r-00003-e02e56d5-d522-4b93-a7f2-f2dc1b2fdba9.gz.parquet  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/binary-classification/part-r-00004-e02e56d5-d522-4b93-a7f2-f2dc1b2fdba9.gz.parquet  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/binary-classification/part-r-00005-e02e56d5-d522-4b93-a7f2-f2dc1b2fdba9.gz.parquet  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/binary-classification/part-r-00006-e02e56d5-d522-4b93-a7f2-f2dc1b2fdba9.gz.parquet  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/binary-classification/part-r-00007-e02e56d5-d522-4b93-a7f2-f2dc1b2fdba9.gz.parquet  \n   creating: /tmp/Spark-The-Definitive-Guide-master/data/clustering/\n extracting: /tmp/Spark-The-Definitive-Guide-master/data/clustering/_SUCCESS  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/clustering/part-r-00000-8891f92d-5542-4aec-a830-0d4ff6f5f871.gz.parquet  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/clustering/part-r-00001-8891f92d-5542-4aec-a830-0d4ff6f5f871.gz.parquet  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/clustering/part-r-00002-8891f92d-5542-4aec-a830-0d4ff6f5f871.gz.parquet  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/clustering/part-r-00003-8891f92d-5542-4aec-a830-0d4ff6f5f871.gz.parquet  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/clustering/part-r-00004-8891f92d-5542-4aec-a830-0d4ff6f5f871.gz.parquet  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/clustering/part-r-00005-8891f92d-5542-4aec-a830-0d4ff6f5f871.gz.parquet  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/clustering/part-r-00006-8891f92d-5542-4aec-a830-0d4ff6f5f871.gz.parquet  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/clustering/part-r-00007-8891f92d-5542-4aec-a830-0d4ff6f5f871.gz.parquet  \n   creating: /tmp/Spark-The-Definitive-Guide-master/data/deep-learning-images/\n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/deep-learning-images/LICENSE.txt  \n   creating: /tmp/Spark-The-Definitive-Guide-master/data/deep-learning-images/daisy/\n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/deep-learning-images/daisy/100080576_f52e8ee070_n.jpg  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/deep-learning-images/daisy/10140303196_b88d3d6cec.jpg  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/deep-learning-images/daisy/10172379554_b296050f82_n.jpg  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/deep-learning-images/daisy/10172567486_2748826a8b.jpg  \n  inflatin\n\n*** WARNING: max output size exceeded, skipping output. ***\n\nating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-03-27.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-03-28.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-03-29.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-03-30.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-03-31.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-04-01.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-04-03.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-04-04.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-04-05.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-04-06.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-04-07.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-04-08.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-04-10.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-04-11.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-04-12.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-04-13.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-04-14.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-04-15.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-04-17.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-04-18.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-04-19.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-04-20.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-04-21.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-04-26.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-04-27.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-04-28.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-01.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-03.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-04.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-05.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-06.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-08.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-09.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-10.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-11.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-12.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-13.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-15.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-16.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-17.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-18.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-19.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-20.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-22.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-23.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-24.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-25.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-26.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-27.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-29.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-31.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-01.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-02.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-03.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-05.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-06.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-07.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-08.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-09.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-10.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-12.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-13.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-14.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-15.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-16.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-17.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-19.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-20.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-21.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-22.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-23.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-24.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-26.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-27.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-28.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-29.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-30.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-01.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-03.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-04.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-05.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-06.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-07.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-08.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-10.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-11.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-12.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-13.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-14.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-15.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-17.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-18.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-19.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-20.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-21.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-22.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-24.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-25.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-26.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-27.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-28.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-29.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-31.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-01.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-02.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-03.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-04.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-05.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-07.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-08.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-09.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-10.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-11.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-12.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-14.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-15.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-16.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-17.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-18.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-19.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-21.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-22.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-23.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-24.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-25.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-26.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-28.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-30.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-31.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-01.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-02.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-04.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-05.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-06.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-07.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-08.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-09.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-11.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-12.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-13.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-14.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-15.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-16.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-18.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-19.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-20.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-21.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-22.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-23.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-25.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-26.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-27.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-28.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-29.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-30.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-02.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-03.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-04.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-05.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-06.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-07.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-09.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-10.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-11.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-12.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-13.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-14.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-16.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-17.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-18.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-19.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-20.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-21.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-23.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-24.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-25.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-26.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-27.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-28.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-30.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-31.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-01.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-02.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-03.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-04.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-06.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-07.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-08.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-09.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-10.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-11.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-13.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-14.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-15.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-16.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-17.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-18.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-20.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-21.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-22.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-23.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-24.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-25.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-27.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-28.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-29.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-30.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-12-01.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-12-02.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-12-04.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-12-05.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-12-06.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-12-07.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-12-08.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-12-09.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/sample_libsvm_data.txt  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/sample_movielens_ratings.txt  \n   creating: /tmp/Spark-The-Definitive-Guide-master/data/simple-ml-integers/\n extracting: /tmp/Spark-The-Definitive-Guide-master/data/simple-ml-integers/_SUCCESS  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/simple-ml-integers/part-00000-ce2a44c8-feb4-4369-a2c3-4bf2f0e63b07-c000.gz.parquet  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/simple-ml-integers/part-00001-ce2a44c8-feb4-4369-a2c3-4bf2f0e63b07-c000.gz.parquet  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/simple-ml-integers/part-00002-ce2a44c8-feb4-4369-a2c3-4bf2f0e63b07-c000.gz.parquet  \n   creating: /tmp/Spark-The-Definitive-Guide-master/data/simple-ml-scaling/\n extracting: /tmp/Spark-The-Definitive-Guide-master/data/simple-ml-scaling/_SUCCESS  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/simple-ml-scaling/part-00000-cd03406a-cc9b-42b0-9299-1e259fdd9382-c000.gz.parquet  \n   creating: /tmp/Spark-The-Definitive-Guide-master/data/simple-ml/\n extracting: /tmp/Spark-The-Definitive-Guide-master/data/simple-ml/_SUCCESS  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/simple-ml/part-r-00000-f5c243b9-a015-4a3b-a4a8-eca00f80f04c.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/license.md  \n   creating: /tmp/Spark-The-Definitive-Guide-master/project-templates/\n  inflating: /tmp/Spark-The-Definitive-Guide-master/project-templates/.gitignore  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/project-templates/README.md  \n   creating: /tmp/Spark-The-Definitive-Guide-master/project-templates/example-data/\n  inflating: /tmp/Spark-The-Definitive-Guide-master/project-templates/example-data/data.json  \n   creating: /tmp/Spark-The-Definitive-Guide-master/project-templates/java/\n extracting: /tmp/Spark-The-Definitive-Guide-master/project-templates/java/.gitignore  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/project-templates/java/example.iml  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/project-templates/java/pom.xml  \n   creating: /tmp/Spark-The-Definitive-Guide-master/project-templates/java/src/\n   creating: /tmp/Spark-The-Definitive-Guide-master/project-templates/java/src/main/\n   creating: /tmp/Spark-The-Definitive-Guide-master/project-templates/java/src/main/java/\n   creating: /tmp/Spark-The-Definitive-Guide-master/project-templates/java/src/main/java/com/\n   creating: /tmp/Spark-The-Definitive-Guide-master/project-templates/java/src/main/java/com/databricks/\n   creating: /tmp/Spark-The-Definitive-Guide-master/project-templates/java/src/main/java/com/databricks/example/\n  inflating: /tmp/Spark-The-Definitive-Guide-master/project-templates/java/src/main/java/com/databricks/example/SimpleExample.java  \n   creating: /tmp/Spark-The-Definitive-Guide-master/project-templates/python/\n   creating: /tmp/Spark-The-Definitive-Guide-master/project-templates/python/pyspark_template/\n extracting: /tmp/Spark-The-Definitive-Guide-master/project-templates/python/pyspark_template/__init__.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/project-templates/python/pyspark_template/main.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/project-templates/python/setup.py  \n   creating: /tmp/Spark-The-Definitive-Guide-master/project-templates/scala/\n  inflating: /tmp/Spark-The-Definitive-Guide-master/project-templates/scala/.gitignore  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/project-templates/scala/build.sbt  \n   creating: /tmp/Spark-The-Definitive-Guide-master/project-templates/scala/src/\n   creating: /tmp/Spark-The-Definitive-Guide-master/project-templates/scala/src/main/\n   creating: /tmp/Spark-The-Definitive-Guide-master/project-templates/scala/src/main/resources/\n  inflating: /tmp/Spark-The-Definitive-Guide-master/project-templates/scala/src/main/resources/log4j.properties  \n   creating: /tmp/Spark-The-Definitive-Guide-master/project-templates/scala/src/main/scala/\n  inflating: /tmp/Spark-The-Definitive-Guide-master/project-templates/scala/src/main/scala/DataFrameExample.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/project-templates/scala/src/main/scala/DatasetExample.scala  \n   creating: /tmp/Spark-The-Definitive-Guide-master/project-templates/scala/src/test/\n   creating: /tmp/Spark-The-Definitive-Guide-master/project-templates/scala/src/test/resources/\n  inflating: /tmp/Spark-The-Definitive-Guide-master/project-templates/scala/src/test/resources/log4j.properties  \n   creating: /tmp/Spark-The-Definitive-Guide-master/project-templates/scala/src/test/scala/\n  inflating: /tmp/Spark-The-Definitive-Guide-master/project-templates/scala/src/test/scala/DataFrameExampleTest.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/project-templates/scala/src/test/scala/DatasetExampleTest.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/project-templates/scala/src/test/scala/TestBase.scala  \n"
     ]
    }
   ],
   "source": [
    "%sh unzip /tmp/Spark_The_Definitive_Guide_master.zip -d /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bad7e39-bff2-45e2-b02c-3006a4dd6775",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/tmp/Spark-The-Definitive-Guide-master/data/flight-data/csv/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%sh ls /tmp/Spark-The-Definitive-Guide-master/data/flight-data/csv/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c5ef8c7-11a8-4a2e-9475-136f70a6bbc7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "\n",
    "flightSchema = StructType([\n",
    "        StructField(\"DEST_COUNTRY_NAME\",StringType(),True),\n",
    "        StructField(\"ORIGIN_COUNTRY_NAME\",StringType(),True),\n",
    "        StructField(\"count\",IntegerType(),True)\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a133e22-0e0d-41b4-aa48-8b80d8f62b92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flight2015 = spark.read.option(\"header\", \"true\").schema(flightSchema).csv(\"file:///tmp/Spark-The-Definitive-Guide-master/data/flight-data/csv/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "836258ef-7241-487c-8bd8-22db74616f98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[11]: 1502"
     ]
    }
   ],
   "source": [
    "flight2015.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb899250-de0f-4812-9460-5675943c8116",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n<class 'tuple'>\n+------+------+------+-----------------+\n|RollNo| FName| LName|           Scores|\n+------+------+------+-----------------+\n|     1|Uddhav|Savani|[20, 25, 30, PCM]|\n|     2|   Dev| Patel|[22, 22, 39, PCM]|\n+------+------+------+-----------------+\n\nroot\n |-- RollNo: long (nullable = true)\n |-- FName: string (nullable = true)\n |-- LName: string (nullable = true)\n |-- Scores: array (nullable = true)\n |    |-- element: string (containsNull = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import ArrayType, StringType, LongType, StructType, StructField\n",
    "smapleListSchema = StructType([\n",
    "    StructField(\"RollNo\", LongType(), True),\n",
    "    StructField(\"FName\", StringType(), True),\n",
    "    StructField(\"LName\", StringType(), True),\n",
    "    StructField(\"Scores\", ArrayType(StringType()), True)\n",
    "])\n",
    "\n",
    "sampleList = [\n",
    "    (1, \"Uddhav\", \"Savani\", (20,\"25\",\"30\",\"PCM\")),\n",
    "    (2, \"Dev\", \"Patel\", (22,\"22\",\"39\",\"PCM\")),\n",
    "]\n",
    "print (type(sampleList))\n",
    "print (type(sampleList[0]))\n",
    "sampleDF = spark.createDataFrame(data=sampleList, schema=smapleListSchema)\n",
    "sampleDF.show()\n",
    "sampleDF.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "248f9efd-65b0-464a-a792-a334d1267186",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n<class 'tuple'>\n+------+------+------+---------+\n|RollNo| FName| LName|    Score|\n+------+------+------+---------+\n|     1|Uddhav|Savani|{P -> 20}|\n|     2|   Dev| Patel|{P -> 23}|\n|     1|Uddhav|Savani|{M -> 21}|\n|     2|   Dev| Patel|{M -> 25}|\n|     1|Uddhav|Savani|{C -> 50}|\n|     2|   Dev| Patel|{C -> 80}|\n|     2|   Dev| Patel|     null|\n+------+------+------+---------+\n\nroot\n |-- RollNo: long (nullable = true)\n |-- FName: string (nullable = true)\n |-- LName: string (nullable = true)\n |-- Score: map (nullable = true)\n |    |-- key: string\n |    |-- value: long (valueContainsNull = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import ArrayType, StringType, LongType, StructType, StructField, MapType\n",
    "smapleListSchema = StructType([\n",
    "    StructField(\"RollNo\", LongType(), True),\n",
    "    StructField(\"FName\", StringType(), True),\n",
    "    StructField(\"LName\", StringType(), True),\n",
    "    StructField(\"Score\", MapType(StringType(), LongType()), True)\n",
    "])\n",
    "\n",
    "sampleList = [\n",
    "    (1, \"Uddhav\", \"Savani\", {\"P\" : 20}),\n",
    "    (2, \"Dev\", \"Patel\", {\"P\" :23}),\n",
    "    (1, \"Uddhav\", \"Savani\", {\"M\" : 21}),\n",
    "    (2, \"Dev\", \"Patel\", {\"M\" :25}),\n",
    "    (1, \"Uddhav\", \"Savani\", {\"C\" : 50}),\n",
    "    (2, \"Dev\", \"Patel\", {\"C\" :80}),\n",
    "    (2, \"Dev\", \"Patel\", None)\n",
    "]\n",
    "print (type(sampleList))\n",
    "print (type(sampleList[0]))\n",
    "sampleDF = spark.createDataFrame(data=sampleList, schema=smapleListSchema)\n",
    "sampleDF.show()\n",
    "sampleDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eacb7d49-0fb9-4c7e-928e-ef5a467a1cc5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n<class 'tuple'>\n+------+------+------+---------+\n|RollNo| FName| LName|    Score|\n+------+------+------+---------+\n|     1|Uddhav|Savani|{P -> 20}|\n|     2|   Dev| Patel|{P -> 23}|\n|     1|Uddhav|Savani|{M -> 21}|\n|     2|   Dev| Patel|{M -> 25}|\n|     1|Uddhav|Savani|{C -> 50}|\n|     2|   Dev| Patel|{C -> 80}|\n|     2|   Dev| Patel|     null|\n+------+------+------+---------+\n\nroot\n |-- RollNo: long (nullable = true)\n |-- FName: string (nullable = true)\n |-- LName: string (nullable = true)\n |-- Score: map (nullable = true)\n |    |-- key: string\n |    |-- value: long (valueContainsNull = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import ArrayType, StringType, LongType, StructType, StructField, MapType\n",
    "smapleListSchema = StructType([\n",
    "    StructField(\"RollNo\", LongType(), True),\n",
    "    StructField(\"FName\", StringType(), True),\n",
    "    StructField(\"LName\", StringType(), True),\n",
    "    StructField(\"Score\", MapType(StringType(), LongType()), True)\n",
    "])\n",
    "\n",
    "sampleList = [\n",
    "    (1, \"Uddhav\", \"Savani\", {\"P\" : 20}),\n",
    "    (2, \"Dev\", \"Patel\", {\"P\" :23}),\n",
    "    (1, \"Uddhav\", \"Savani\", {\"M\" : 21}),\n",
    "    (2, \"Dev\", \"Patel\", {\"M\" :25}),\n",
    "    (1, \"Uddhav\", \"Savani\", {\"C\" : 50}),\n",
    "    (2, \"Dev\", \"Patel\", {\"C\" :80}),\n",
    "    (2, \"Dev\", \"Patel\", None)\n",
    "]\n",
    "print (type(sampleList))\n",
    "print (type(sampleList[0]))\n",
    "sampleDF = spark.createDataFrame(data=sampleList, schema=smapleListSchema)\n",
    "sampleDF.show()\n",
    "sampleDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "379b7dec-bf1f-403c-9e35-c88fe1125225",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+--------------------+-------------------+-----+\n|       United States|            Romania|   15|\n|       United States|            Croatia|    1|\n|       United States|            Ireland|  344|\n|               Egypt|      United States|   15|\n|       United States|              India|   62|\n|       United States|          Singapore|    1|\n|       United States|            Grenada|   62|\n|          Costa Rica|      United States|  588|\n|             Senegal|      United States|   40|\n|             Moldova|      United States|    1|\n|       United States|       Sint Maarten|  325|\n|       United States|   Marshall Islands|   39|\n|              Guyana|      United States|   64|\n|               Malta|      United States|    1|\n|            Anguilla|      United States|   41|\n|             Bolivia|      United States|   30|\n|       United States|           Paraguay|    6|\n|             Algeria|      United States|    4|\n|Turks and Caicos ...|      United States|  230|\n|       United States|          Gibraltar|    1|\n+--------------------+-------------------+-----+\nonly showing top 20 rows\n\nroot\n |-- DEST_COUNTRY_NAME: string (nullable = true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n |-- count: long (nullable = true)\n\nOut[4]: StructType([StructField('DEST_COUNTRY_NAME', StringType(), True), StructField('ORIGIN_COUNTRY_NAME', StringType(), True), StructField('count', LongType(), True)])"
     ]
    }
   ],
   "source": [
    "flightDFFromJson = spark.read.format(\"json\").load(\"file:///tmp/Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json\")\n",
    "flightDFFromJson.show()\n",
    "flightDFFromJson.printSchema()\n",
    "flightDFFromJson.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0c152c8-753e-48fe-8c42-6930f913bb96",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-------+\n|CustomerID| FName|  LName|\n+----------+------+-------+\n|         1|Uddhav| Savani|\n|         2|   Dev|  Patel|\n|         3|  Tony|  Stark|\n|         4|  Thor|Odinson|\n+----------+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import ArrayType, StringType, LongType, StructType, StructField, MapType, DateType\n",
    "CustomerSchema = StructType([\n",
    "    StructField(\"CustomerID\", LongType(), True),\n",
    "    StructField(\"FName\", StringType(), True),\n",
    "    StructField(\"LName\", StringType(), True)\n",
    "])\n",
    "\n",
    "CustomerList = [\n",
    "    (1, \"Uddhav\", \"Savani\"),\n",
    "    (2, \"Dev\", \"Patel\"),\n",
    "    (3, \"Tony\", \"Stark\"),\n",
    "    (4, \"Thor\", \"Odinson\")\n",
    "]\n",
    "\n",
    "customerDF = spark.createDataFrame(data=CustomerList, schema=CustomerSchema)\n",
    "customerDF.show()\n",
    "\n",
    "OrderSchema = StructType([\n",
    "    StructField(\"OrderID\", LongType(), True),\n",
    "    StructField(\"OrderDate\", DateType(), True),\n",
    "    StructField(\"ProductID\", LongType(), True),\n",
    "    StructField(\"CustomerID\", LongType(), True)\n",
    "])\n",
    "\n",
    "OrderList = [\n",
    "    (1001, \"2024-02-11\", 276, 1),\n",
    "    (1004, \"2024-02-12\", 5234, 2),\n",
    "    (1005, \"2024-02-13\", 198, 3),\n",
    "    (1009, \"2024-02-14\", 8210, 4),\n",
    "    (1015, \"2024-02-15\", 128, 5),\n",
    "]\n",
    "\n",
    "orderDF = spark.createDataFrame(data=OrderList, schema=OrderSchema)\n",
    "orderDF.show()\n",
    "#sampleDF.col(\"FName\") To Reference A Column In Join and Other Requirenments\n",
    "#col(\"Fname\") Can also Be used In this Case\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d1af5d5-92d4-4219-9b9a-be5a21980625",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-------+\n|CustomerID| FName|  LName|\n+----------+------+-------+\n|         1|Uddhav| Savani|\n|         2|   Dev|  Patel|\n|         3|  Tony|  Stark|\n|         4|  Thor|Odinson|\n+----------+------+-------+\n\n+-------+----------+---------+----------+\n|OrderID| OrderDate|ProductID|CustomerID|\n+-------+----------+---------+----------+\n|   1001|2024-02-11|      276|         1|\n|   1004|2024-02-12|     5234|         2|\n|   1005|2024-02-13|      198|         3|\n|   1009|2024-02-14|     8210|         4|\n|   1015|2024-02-15|      128|         5|\n+-------+----------+---------+----------+\n\n+-------+----------+---------+----------+----------+------+-------+\n|OrderID| OrderDate|ProductID|CustomerID|CustomerID| FName|  LName|\n+-------+----------+---------+----------+----------+------+-------+\n|   1001|2024-02-11|      276|         1|         1|Uddhav| Savani|\n|   1004|2024-02-12|     5234|         2|         2|   Dev|  Patel|\n|   1005|2024-02-13|      198|         3|         3|  Tony|  Stark|\n|   1009|2024-02-14|     8210|         4|         4|  Thor|Odinson|\n|   1015|2024-02-15|      128|         5|      null|  null|   null|\n+-------+----------+---------+----------+----------+------+-------+\n\n+-------+----------+---------+----------+----------+------+-------+\n|OrderID| OrderDate|ProductID|CustomerID|CustomerID| FName|  LName|\n+-------+----------+---------+----------+----------+------+-------+\n|   1001|2024-02-11|      276|         1|         1|Uddhav| Savani|\n|   1004|2024-02-12|     5234|         2|         2|   Dev|  Patel|\n|   1005|2024-02-13|      198|         3|         3|  Tony|  Stark|\n|   1009|2024-02-14|     8210|         4|         4|  Thor|Odinson|\n|   1015|2024-02-15|      128|         5|      null|  null|   null|\n+-------+----------+---------+----------+----------+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import ArrayType, StringType, LongType, StructType, StructField, MapType, DateType\n",
    "from datetime import date\n",
    "CustomerSchema = StructType([\n",
    "    StructField(\"CustomerID\", LongType(), True),\n",
    "    StructField(\"FName\", StringType(), True),\n",
    "    StructField(\"LName\", StringType(), True)\n",
    "])\n",
    "\n",
    "CustomerList = [\n",
    "    (1, \"Uddhav\", \"Savani\"),\n",
    "    (2, \"Dev\", \"Patel\"),\n",
    "    (3, \"Tony\", \"Stark\"),\n",
    "    (4, \"Thor\", \"Odinson\")\n",
    "]\n",
    "\n",
    "customerDF = spark.createDataFrame(data=CustomerList, schema=CustomerSchema)\n",
    "customerDF.show()\n",
    "\n",
    "OrderSchema = StructType([\n",
    "    StructField(\"OrderID\", LongType(), True),\n",
    "    StructField(\"OrderDate\", DateType(), True),\n",
    "    StructField(\"ProductID\", LongType(), True),\n",
    "    StructField(\"CustomerID\", LongType(), True)\n",
    "])\n",
    "\n",
    "# OrderList = [\n",
    "#     (1001, date(\"2024-02-11\"), 276, 1),\n",
    "#     (1004, date(\"2024-02-12\"), 5234, 2),\n",
    "#     (1005, date(\"2024-02-13\"), 198, 3),\n",
    "#     (1009, date(\"2024-02-14\"), 8210, 4),\n",
    "#     (1015, date(\"2024-02-15\"), 128, 5),\n",
    "# ]\n",
    "\n",
    "#      27 OrderList = [\n",
    "# ---> 28     (1001, date(\"2024-02-11\"), 276, 1),\n",
    "#      29     (1004, date(\"2024-02-12\"), 5234, 2),\n",
    "#      30     (1005, date(\"2024-02-13\"), 198, 3),\n",
    "#      31     (1009, date(\"2024-02-14\"), 8210, 4),\n",
    "#      32     (1015, date(\"2024-02-15\"), 128, 5),\n",
    "#      33 ]\n",
    "#      35 orderDF = spark.createDataFrame(data=OrderList, schema=OrderSchema)\n",
    "#      36 orderDF.show()\n",
    "\n",
    "# TypeError: an integer is required (got type str)\n",
    "\n",
    "OrderList = [\n",
    "    (1001, date(2024,2,11), 276, 1),\n",
    "    (1004, date(2024,2,12), 5234, 2),\n",
    "    (1005, date(2024,2,13), 198, 3),\n",
    "    (1009, date(2024,2,14), 8210, 4),\n",
    "    (1015, date(2024,2,15), 128, 5),\n",
    "]\n",
    "\n",
    "orderDF = spark.createDataFrame(data=OrderList, schema=OrderSchema)\n",
    "orderDF.show()\n",
    "\n",
    "enrichOrder = orderDF.join(customerDF, orderDF.CustomerID == customerDF.CustomerID, how=\"left\")\n",
    "enrichOrder.show()\n",
    "\n",
    "enrichOrder2 = orderDF.join(customerDF, orderDF[\"CustomerID\"] == customerDF[\"CustomerID\"], how=\"left\")\n",
    "enrichOrder2.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51f1a726-2400-416c-a344-a18a8ac8a3a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|(id * 5)|\n+--------+\n|       0|\n|       5|\n|      10|\n|      15|\n|      20|\n+--------+\nonly showing top 5 rows\n\n+--------+\n|(id * 5)|\n+--------+\n|       0|\n|       5|\n|      10|\n|      15|\n|      20|\n+--------+\nonly showing top 5 rows\n\n+---+----+\n| id|mul5|\n+---+----+\n|  0|   0|\n|  1|   5|\n|  2|  10|\n|  3|  15|\n|  4|  20|\n+---+----+\nonly showing top 5 rows\n\n+---+----+\n| id|mul5|\n+---+----+\n|  0|   0|\n|  1|   5|\n|  2|  10|\n|  3|  15|\n|  4|  20|\n+---+----+\nonly showing top 5 rows\n\n+--------+\n|(id * 5)|\n+--------+\n|       0|\n|       5|\n|      10|\n|      15|\n|      20|\n+--------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "# Expressions in Pyspark\n",
    "# expr(\"someCol - 5\") == col(\"someCol\") - 5 == expr(\"someCol\") - 5\n",
    "# (((col(\"someCol\") + 5) * 200) - 6) < col(\"otherCol\") == expr(\"(((someCol + 5) * 200) - 6) < otherCol\")\n",
    "\n",
    "_1000range = spark.range(1000)\n",
    "_1000range\n",
    "_5000range = _1000range.select(_1000range.id * 5)\n",
    "_5000range.show(5)\n",
    "_5000range1 = _1000range.select(_1000range[\"id\"] * 5)\n",
    "_5000range1.show(5)\n",
    "\n",
    "# Belov Will Fail Or It may not be able to resolve the id.\n",
    "# _5000range2 = expr(\"id * 5\")\n",
    "# _5000range2.show(5)\n",
    "\n",
    "_1000rangeWithMul5 = _1000range.withColumn(\"mul5\",expr(\"id * 5\"))\n",
    "_1000rangeWithMul5.show(5)\n",
    "\n",
    "_1000rangeWithMul5WithCol = _1000range.withColumn(\"mul5\",_1000range.id * 5)\n",
    "_1000rangeWithMul5WithCol.show(5)\n",
    "\n",
    "_1000rangeWithMul5WithSelect = _1000range.select(_1000range.id * 5)\n",
    "_1000rangeWithMul5WithSelect.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0c083f2-990d-422a-8ef4-d03104bbc569",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n|    United States|            Ireland|  344|\n|            Egypt|      United States|   15|\n|    United States|              India|   62|\n+-----------------+-------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Create A Temporary Table FOr Querying Using SQL\n",
    "df = spark.read.format(\"json\").load(\"file:///tmp/Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json\")\n",
    "df.createOrReplaceTempView(\"dfTable\")\n",
    "dfshow = spark.sql(\"select * from dfTable limit 5\")\n",
    "dfshow.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f773e07-594a-435b-ba64-a48828717ec9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n| some| col|names|\n+-----+----+-----+\n|Hello|null|    1|\n+-----+----+-----+\n\n+------+----+-----+\n|  some| col|names|\n+------+----+-----+\n| Hello|null|    1|\n|Hello2|null|    2|\n+------+----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "myManualSchema = StructType([\n",
    "StructField(\"some\", StringType(), True),\n",
    "StructField(\"col\", StringType(), True),\n",
    "StructField(\"names\", LongType(), False)\n",
    "])\n",
    "myRow = Row(\"Hello\", None, 1)\n",
    "myDf = spark.createDataFrame([myRow], myManualSchema)\n",
    "myDf.show()\n",
    "\n",
    "myRow = [\n",
    "    Row(\"Hello\", None, 1),\n",
    "    Row(\"Hello2\", None, 2),\n",
    "]\n",
    "myDf2 = spark.createDataFrame(myRow, myManualSchema)\n",
    "myDf2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ff07b49-f118-46d7-a390-5bb65dde2e5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- DEST_COUNTRY_NAME: string (nullable = true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n |-- count: long (nullable = true)\n\n+-----------------+-----+\n|DEST_COUNTRY_NAME|count|\n+-----------------+-----+\n|    United States|   15|\n|    United States|    1|\n|    United States|  344|\n|            Egypt|   15|\n|    United States|   62|\n+-----------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"json\").load(\"file:///tmp/Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json\")\n",
    "df.printSchema()\n",
    "df.select(\"DEST_COUNTRY_NAME\",\"count\").show(5)\n",
    "# df.show()\n",
    "# dfshow = df.select()\n",
    "# dfshow.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d0ab146-6e32-4ab6-b7ea-5124f2ae8c99",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- DEST_COUNTRY_NAME: string (nullable = true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n |-- count: long (nullable = true)\n\n+-----------------+-----------------+-----------------+\n|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|\n+-----------------+-----------------+-----------------+\n|    United States|    United States|    United States|\n|    United States|    United States|    United States|\n+-----------------+-----------------+-----------------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, col, column\n",
    "df = spark.read.format(\"json\").load(\"file:///tmp/Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json\")\n",
    "df.printSchema()\n",
    "df.select(\n",
    "expr(\"DEST_COUNTRY_NAME\"),\n",
    "col(\"DEST_COUNTRY_NAME\"),\n",
    "column(\"DEST_COUNTRY_NAME\"))\\\n",
    ".show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "101025ea-66d1-4e12-a915-c91fb8b7d97a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- DEST_COUNTRY_NAME: string (nullable = true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n |-- count: long (nullable = true)\n\n+-----------------+-----------------+\n|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|\n+-----------------+-----------------+\n|    United States|    United States|\n|    United States|    United States|\n|    United States|    United States|\n|            Egypt|            Egypt|\n|    United States|    United States|\n+-----------------+-----------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, col, column\n",
    "df = spark.read.format(\"json\").load(\"file:///tmp/Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json\")\n",
    "df.printSchema()\n",
    "df.select(col(\"DEST_COUNTRY_NAME\"), \"DEST_COUNTRY_NAME\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9221135-a549-417c-bfed-c1918785a1c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n|  destination|\n+-------------+\n|United States|\n|United States|\n+-------------+\nonly showing top 2 rows\n\n+-----------------+\n|DEST_COUNTRY_NAME|\n+-----------------+\n|    United States|\n|    United States|\n+-----------------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, col, column\n",
    "df = spark.read.format(\"json\").load(\"file:///tmp/Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json\")\n",
    "df.select(expr(\"DEST_COUNTRY_NAME AS destination\")).show(2)\n",
    "df.select(expr(\"DEST_COUNTRY_NAME as destination\").alias(\"DEST_COUNTRY_NAME\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35a1d4bf-151e-4fb2-ba62-f0cf3902e063",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n+-----------------+-------------------+-----+-------------+\n|    United States|            Romania|   15|        false|\n|    United States|            Croatia|    1|        false|\n+-----------------+-------------------+-----+-------------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "# As Select After Expr is Very Common Use Case Spark Provides selectExpr\n",
    "df = spark.read.format(\"json\").load(\"file:///tmp/Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json\")\n",
    "df.selectExpr(\n",
    "\"*\", # all original columns\n",
    "\"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\")\\\n",
    ".show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29a5f93f-51b9-4661-83aa-f8431f306e28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------------+\n| avg(count)|count(DISTINCT DEST_COUNTRY_NAME)|\n+-----------+---------------------------------+\n|1770.765625|                              132|\n+-----------+---------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Aggrigate Function In selectExpr\n",
    "df = spark.read.format(\"json\").load(\"file:///tmp/Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json\")\n",
    "df.selectExpr(\"avg(count)\", \"count(distinct(DEST_COUNTRY_NAME))\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd8846d1-5168-4ef0-8812-57f2f09f8439",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|\n+-----------------+-------------------+-----+---+\n|    United States|            Romania|   15|  1|\n|    United States|            Croatia|    1|  1|\n+-----------------+-------------------+-----+---+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df = spark.read.format(\"json\").load(\"file:///tmp/Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json\")\n",
    "df.select(expr(\"*\"), lit(1).alias(\"One\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdf57c33-96e0-4323-873e-54e24ca86066",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|ONE|\n+-----------------+-------------------+-----+---+\n|    United States|            Romania|   15|  1|\n|    United States|            Croatia|    1|  1|\n|    United States|            Ireland|  344|  1|\n|            Egypt|      United States|   15|  1|\n|    United States|              India|   62|  1|\n+-----------------+-------------------+-----+---+\nonly showing top 5 rows\n\n+-----------------+-------------------+-----+------------------------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|SameOriginAndDestination|\n+-----------------+-------------------+-----+------------------------+\n|    United States|            Romania|   15|                   false|\n|    United States|            Croatia|    1|                   false|\n|    United States|            Ireland|  344|                   false|\n|            Egypt|      United States|   15|                   false|\n|    United States|              India|   62|                   false|\n+-----------------+-------------------+-----+------------------------+\nonly showing top 5 rows\n\n+-----------------+-------------------+-----+------------------------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|SameOriginAndDestination|\n+-----------------+-------------------+-----+------------------------+\n|    United States|            Romania|   15|                   false|\n|    United States|            Croatia|    1|                   false|\n|    United States|            Ireland|  344|                   false|\n|            Egypt|      United States|   15|                   false|\n|    United States|              India|   62|                   false|\n+-----------------+-------------------+-----+------------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Methods To Add Columns IN DF\n",
    "from pyspark.sql.functions import lit\n",
    "df = spark.read.format(\"json\").load(\"file:///tmp/Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json\")\n",
    "df.withColumn(\"ONE\", lit(1)).show(5)\n",
    "\n",
    "df.withColumn(\"SameOriginAndDestination\", df.DEST_COUNTRY_NAME == df.ORIGIN_COUNTRY_NAME).show(5)\n",
    "\n",
    "df.withColumn(\"SameOriginAndDestination\", expr(\"DEST_COUNTRY_NAME == ORIGIN_COUNTRY_NAME\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67964559-9211-441e-a19b-7ba0852fbf0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- DEST_COUNTRY_NAME: string (nullable = true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n |-- count: long (nullable = true)\n\n+-------------------+-------------------+-----+\n|DESTINATION_RENAMED|ORIGIN_COUNTRY_NAME|count|\n+-------------------+-------------------+-----+\n|      United States|            Romania|   15|\n|      United States|            Croatia|    1|\n|      United States|            Ireland|  344|\n|              Egypt|      United States|   15|\n|      United States|              India|   62|\n+-------------------+-------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Rename a Column\n",
    "# 1) By Adding New Column using withColumn utility => It will add additional Column\n",
    "# 2) By using withColumnRenamed Utility => Will actually rename the column\n",
    "\n",
    "df = spark.read.format(\"json\").load(\"file:///tmp/Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json\")\n",
    "df.printSchema()\n",
    "\n",
    "df.withColumnRenamed(\"DEST_COUNTRY_NAME\",\"DESTINATION_RENAMED\").show(5)\n",
    "\n",
    "# df.withColumnRenamed(df.DEST_COUNTRY_NAME,\"DESTINATION_RENAMED\").show(5) \n",
    "# Will Give Error, As per API DOC \n",
    "# DataFrame.withColumnRenamed(existing, new)\n",
    "\n",
    "    # existingstr\n",
    "\n",
    "    #     string, name of the existing column to rename.\n",
    "    # newstr\n",
    "\n",
    "    #     string, new name of the column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5983c4d5-5892-4e8b-a5a3-531911442d8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------+\n|This Long Column-Name|new col|\n+---------------------+-------+\n|              Romania|Romania|\n|              Croatia|Croatia|\n+---------------------+-------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Reserved Characters and Keywords\n",
    "df = spark.read.format(\"json\").load(\"file:///tmp/Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json\")\n",
    "dfWithLongColName = df.withColumn(\"This Long Column-Name\", expr(\"ORIGIN_COUNTRY_NAME\"))\n",
    "\n",
    "dfWithLongColName.selectExpr(\n",
    "\"`This Long Column-Name`\",\n",
    "\"`This Long Column-Name` as `new col`\")\\\n",
    ".show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b5cb135-8ed8-4410-87b1-4a44a7aee3b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# By default Spark is case insensitive; however, you can make Spark case sensitive by setting the\n",
    "# configuration:\n",
    "# -- in SQL\n",
    "# set spark.sql.caseSensitive true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cba7395f-8b6b-4996-9c8d-2bd21506b3ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Removing Columns\n",
    "# df.drop(\"ORIGIN_COUNTRY_NAME\")\n",
    "# dfWithLongColName.drop(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\") # Multiple Column Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe38e376-cebc-495a-ab2a-c04485b74305",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- DEST_COUNTRY_NAME: string (nullable = true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n |-- count: long (nullable = true)\n\nroot\n |-- count: string (nullable = true)\n\nroot\n |-- count: string (nullable = true)\n\n<class 'pyspark.sql.column.Column'>\n<class 'pyspark.sql.column.Column'>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df = spark.read.format(\"json\").load(\"file:///tmp/Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json\")\n",
    "df.printSchema()\n",
    "\n",
    "# df.select(df.count.cast(\"string\")).show() # Will Give Error\n",
    "df.select(df[\"count\"].cast(\"string\")).printSchema()\n",
    "df.select(col(\"count\").cast(\"string\")).printSchema()\n",
    "print(type(df[\"count\"]))\n",
    "# df.select(\"count\").show()\n",
    "print (type(col(\"count\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b63babe0-b26e-4ff8-b597-d887cdfad031",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Croatia|    1|\n|    United States|          Singapore|    1|\n+-----------------+-------------------+-----+\nonly showing top 2 rows\n\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Croatia|    1|\n|    United States|          Singapore|    1|\n+-----------------+-------------------+-----+\nonly showing top 2 rows\n\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Croatia|    1|\n|    United States|          Singapore|    1|\n+-----------------+-------------------+-----+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Filtering Rows\n",
    "# 1) using where\n",
    "# 2) using filter\n",
    "from pyspark.sql.functions import col\n",
    "df = spark.read.format(\"json\").load(\"file:///tmp/Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json\")\n",
    "df.filter(col(\"count\") < 2).show(2)\n",
    "df.where(\"count < 2\").show(2)\n",
    "df.where(df[\"count\"] < 2).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdc8f197-6aeb-4493-a4af-696a96996e5d",
     "showTitle": true,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+\n|ORIGIN_COUNTRY_NAME|DEST_COUNTRY_NAME|\n+-------------------+-----------------+\n|            Romania|    United States|\n|            Croatia|    United States|\n|            Ireland|    United States|\n|      United States|            Egypt|\n|              India|    United States|\n+-------------------+-----------------+\nonly showing top 5 rows\n\n256\n256\n"
     ]
    }
   ],
   "source": [
    "# Extracting Unique Rows From Data Frame\n",
    "df = spark.read.format(\"json\").load(\"file:///tmp/Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json\")\n",
    "df.select(\"ORIGIN_COUNTRY_NAME\",\"DEST_COUNTRY_NAME\").distinct().show(5)\n",
    "print(df.select(\"ORIGIN_COUNTRY_NAME\",\"DEST_COUNTRY_NAME\").count())\n",
    "print(df.select(\"ORIGIN_COUNTRY_NAME\",\"DEST_COUNTRY_NAME\").distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3669a5c8-fa9b-45f2-85c2-c7be43fef71d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[12]: 34"
     ]
    }
   ],
   "source": [
    "# Random Samples\n",
    "seed = 5\n",
    "withReplacement = False\n",
    "fraction = 0.1\n",
    "df = spark.read.format(\"json\").load(\"file:///tmp/Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json\")\n",
    "df.sample(withReplacement, fraction, seed).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c48018a4-fedd-40be-8d47-4dee3861bbdf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n71\n185\nOut[18]: True"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"json\").load(\"file:///tmp/Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json\")\n",
    "seed = 5\n",
    "dataFrames = df.randomSplit([0.25, 0.75], seed)\n",
    "print (df.count())\n",
    "print(dataFrames[0].count())\n",
    "print(dataFrames[1].count())\n",
    "dataFrames[0].count() < dataFrames[1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5aa4acef-c343-4da0-af8d-e9e6eed0e28a",
     "showTitle": true,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n|    United States|            Ireland|  344|\n|    United States|              India|   62|\n|    United States|          Singapore|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Concatenating and Appending Rows (Union)\n",
    "# As DFs are immutable, New Rows can not be appended to existing DFs.\n",
    "# To Union DFS Schema and No Of Columns Strictly Match\n",
    "# Unions are currently performed based on location, not on the schema. This means that columns will not automatically line up the way you think they might.\n",
    "\n",
    "df = spark.read.format(\"json\").load(\"file:///tmp/Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json\")\n",
    "\n",
    "US = df.where(df[\"DEST_COUNTRY_NAME\"] == \"United States\")\n",
    "Itly = df.where(df[\"DEST_COUNTRY_NAME\"] == \"Itly\")\n",
    "\n",
    "US.union(Itly).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ef6a3d1-27c0-438c-9b09-609271da1897",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+--------------------+-------------------+-----+\n|               Malta|      United States|    1|\n|Saint Vincent and...|      United States|    1|\n|       United States|            Croatia|    1|\n|       United States|          Gibraltar|    1|\n|       United States|          Singapore|    1|\n+--------------------+-------------------+-----+\nonly showing top 5 rows\n\n+-----------------+-------------------+------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n+-----------------+-------------------+------+\n|    United States|      United States|370002|\n|    United States|             Canada|  8483|\n|           Canada|      United States|  8399|\n|    United States|             Mexico|  7187|\n|           Mexico|      United States|  7140|\n+-----------------+-------------------+------+\nonly showing top 5 rows\n\n+-----------------+-------------------+------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n+-----------------+-------------------+------+\n|    United States|      United States|370002|\n|    United States|             Canada|  8483|\n|           Canada|      United States|  8399|\n|    United States|             Mexico|  7187|\n|           Mexico|      United States|  7140|\n+-----------------+-------------------+------+\nonly showing top 5 rows\n\n+-----------------+-------------------+------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n+-----------------+-------------------+------+\n|    United States|      United States|370002|\n|    United States|             Canada|  8483|\n|           Canada|      United States|  8399|\n|    United States|             Mexico|  7187|\n|           Mexico|      United States|  7140|\n+-----------------+-------------------+------+\nonly showing top 5 rows\n\n+--------------------+-------------------+-----+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+--------------------+-------------------+-----+\n|               Malta|      United States|    1|\n|Saint Vincent and...|      United States|    1|\n|       United States|            Croatia|    1|\n|       United States|          Gibraltar|    1|\n|       United States|          Singapore|    1|\n+--------------------+-------------------+-----+\nonly showing top 5 rows\n\n+-----------------+-------------------+------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n+-----------------+-------------------+------+\n|    United States|      United States|370002|\n|    United States|             Canada|  8483|\n|           Canada|      United States|  8399|\n|    United States|             Mexico|  7187|\n|           Mexico|      United States|  7140|\n+-----------------+-------------------+------+\nonly showing top 5 rows\n\n+-----------------+-------------------+------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n+-----------------+-------------------+------+\n|    United States|      United States|370002|\n|    United States|             Canada|  8483|\n|           Canada|      United States|  8399|\n|    United States|             Mexico|  7187|\n|           Mexico|      United States|  7140|\n+-----------------+-------------------+------+\nonly showing top 5 rows\n\n1\n"
     ]
    }
   ],
   "source": [
    "# Sorting Rows\n",
    "# sort and orderBy Both works exectly same\n",
    "from pyspark.sql.functions import expr\n",
    "df = spark.read.format(\"json\").load(\"file:///tmp/Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json\")\n",
    "\n",
    "df.sort(df[\"count\"]).show(5) # By Default Asc\n",
    "df.sort(df[\"count\"], ascending=False).show(5)\n",
    "df.sort([df[\"count\"], df[\"ORIGIN_COUNTRY_NAME\"], df[\"DEST_COUNTRY_NAME\"]],ascending=[False, True, False]).show(5)\n",
    "df.orderBy([df[\"count\"], df[\"ORIGIN_COUNTRY_NAME\"], df[\"DEST_COUNTRY_NAME\"]],ascending=[False, True, False]).show(5)\n",
    "df.orderBy(expr(\"count desc\")).show(5)\n",
    "df.orderBy(df[\"count\"].desc(), col(\"ORIGIN_COUNTRY_NAME\").asc()).show(5)\n",
    "\n",
    "\n",
    "# An advanced tip is to use asc_nulls_first, desc_nulls_first, asc_nulls_last, or desc_nulls_last to specify where you would like your null values to appear in an ordered DataFrame.\n",
    "# For optimization purposes, its sometimes advisable to sort within each partition before another set of transformations. You can use the sortWithinPartitions method to do this:\n",
    "df.sortWithinPartitions(df[\"count\"].desc()).show(5)\n",
    "\n",
    "# An advanced tip is to use asc_nulls_first, desc_nulls_first, asc_nulls_last, or desc_nulls_last to specify where you would like your null values to appear in an ordered DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1b9d7f7-7b61-4a09-a660-0d2bcb443301",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n|    United States|            Ireland|  344|\n|            Egypt|      United States|   15|\n|    United States|              India|   62|\n+-----------------+-------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Limit\n",
    "df = spark.read.format(\"json\").load(\"file:///tmp/Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json\")\n",
    "df.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "190d849a-ce5e-42e5-a944-a19d5c467588",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n5\n1\n5\n"
     ]
    }
   ],
   "source": [
    "# Repartition and Coalesce\n",
    "# Another important optimization opportunity is to partition the data according to some frequently filtered columns, which control the physical layout of data across the cluster including the partitioning scheme and the number of partitions.\n",
    "# Repartition will incur a full shuffle of the data, regardless of whether one is necessary. This means that you should typically only repartition when the future number of partitions is greater than your current number of partitions or when you are looking to partition by a set of columns\n",
    "df = spark.read.format(\"json\").load(\"file:///tmp/Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json\")\n",
    "\n",
    "print (df.rdd.getNumPartitions())\n",
    "rdf = df.repartition(5)\n",
    "print (rdf.rdd.getNumPartitions())\n",
    "\n",
    "# If you know that youre going to be filtering by a certain column often, it can be worth repartitioning based on that column:\n",
    "print(df.repartition(col(\"DEST_COUNTRY_NAME\")).rdd.getNumPartitions())\n",
    "\n",
    "# Optionally Specify Num Of partition While Partitioning With Column Name\n",
    "print(df.repartition(5,col(\"DEST_COUNTRY_NAME\")).rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9741db8-d153-4c56-ac78-6f2b6e9063ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n2\n"
     ]
    }
   ],
   "source": [
    "# Coalesce\n",
    "# Coalesce, on the other hand, will not incur a full shuffle and will try to combine partitions. This operation will shuffle your data into five partitions based on the destination country name, and then coalesce them (without a full shuffle):\n",
    "df = spark.read.format(\"json\").load(\"file:///tmp/Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json\")\n",
    "rdf = df.repartition(5, col(\"DEST_COUNTRY_NAME\"))\n",
    "print (rdf.rdd.getNumPartitions())\n",
    "crdf = rdf.coalesce(2)\n",
    "print (crdf.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aeefe296-66f6-4255-9f12-478e966b02da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n|    United States|            Ireland|  344|\n|            Egypt|      United States|   15|\n|    United States|              India|   62|\n|    United States|          Singapore|    1|\n|    United States|            Grenada|   62|\n|       Costa Rica|      United States|  588|\n|          Senegal|      United States|   40|\n|          Moldova|      United States|    1|\n+-----------------+-------------------+-----+\n\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|United States    |Romania            |15   |\n|United States    |Croatia            |1    |\n|United States    |Ireland            |344  |\n|Egypt            |United States      |15   |\n|United States    |India              |62   |\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\nOut[51]: [Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344),\n Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Grenada', count=62),\n Row(DEST_COUNTRY_NAME='Costa Rica', ORIGIN_COUNTRY_NAME='United States', count=588),\n Row(DEST_COUNTRY_NAME='Senegal', ORIGIN_COUNTRY_NAME='United States', count=40),\n Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]"
     ]
    }
   ],
   "source": [
    "# Collecting Rows to the Driver\n",
    "# Spark maintains the state of the cluster in the driver. There are times when youll want to collect some of your data to the driver in order to manipulate it on your local machine.\n",
    "\n",
    "# several different methods for doing so that are effectively all the same. collect gets all data from the entire DataFrame, take selects the first N rows, and show prints out a number of rows nicely.\n",
    "df = spark.read.format(\"json\").load(\"file:///tmp/Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json\")\n",
    "collectDF = df.limit(10)\n",
    "collectDF.take(5) # take works with an Integer count\n",
    "collectDF.show() # this prints it out nicely\n",
    "collectDF.show(5, False)\n",
    "collectDF.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2eb32569-b10f-4fbc-93ba-5add6c72cf21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object _local_iterator_from_socket.<locals>.PyLocalIterable.__iter__ at 0x7f76e5c62eb0>\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344)\nRow(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1)\nRow(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Grenada', count=62)\nRow(DEST_COUNTRY_NAME='Costa Rica', ORIGIN_COUNTRY_NAME='United States', count=588)\nRow(DEST_COUNTRY_NAME='Senegal', ORIGIN_COUNTRY_NAME='United States', count=40)\nRow(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)\n"
     ]
    }
   ],
   "source": [
    "# Theres an additional way of collecting rows to the driver in order to iterate over the entire dataset. The method toLocalIterator collects partitions to the driver as an iterator. This method allows you to iterate over the entire dataset partition-by-partition in a serial manner:\n",
    "\n",
    "print(collectDF.toLocalIterator())\n",
    "\n",
    "for item in collectDF.toLocalIterator():\n",
    "    print (item)\n",
    "\n",
    "# Any collection of data to the driver can be a very expensive operation! If you have a large dataset and call collect, you can crash the driver. If you use toLocalIterator and have very large partitions, you can easily crash the driver node and lose the state of your application. This is also expensive because we can operate on a one-by-one basis, instead of running computation in parallel"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3426924950954753,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark The Definative Guide Chapter 5",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
