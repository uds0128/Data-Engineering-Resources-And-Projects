{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf906e00-9187-4def-b53c-25f459ad32e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[16]: True"
     ]
    }
   ],
   "source": [
    "dbutils.fs.cp(\n",
    "  'dbfs:/FileStore/Spark_The_Definitive_Guide_master.zip', \n",
    "  'file:/tmp/Spark_The_Definitive_Guide_master.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e234f91-8bc2-4352-97c5-25f1616c3152",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /tmp/Spark_The_Definitive_Guide_master.zip\n4ba5601eb9b9aed1d01ab79775e3af228216ff6f\n   creating: /tmp/Spark-The-Definitive-Guide-master/\n  inflating: /tmp/Spark-The-Definitive-Guide-master/.gitignore  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/README.md  \n   creating: /tmp/Spark-The-Definitive-Guide-master/code/\n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/A_Gentle_Introduction_to_Spark-Chapter_1_Defining_Spark.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/A_Gentle_Introduction_to_Spark-Chapter_2_A_Gentle_Introduction_to_Spark.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/A_Gentle_Introduction_to_Spark-Chapter_2_A_Gentle_Introduction_to_Spark.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/A_Gentle_Introduction_to_Spark-Chapter_3_A_Tour_of_Sparks_Toolset.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/A_Gentle_Introduction_to_Spark-Chapter_3_A_Tour_of_Sparks_Toolset.r  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/A_Gentle_Introduction_to_Spark-Chapter_3_A_Tour_of_Sparks_Toolset.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/A_Gentle_Introduction_to_Spark_Chapter_2_A_Gentle_Introduction_to_Spark.java  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Advanced_Analytics_and_Machine_Learning-Chapter_24_Advanced_Analytics_and_Machine_Learning.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Advanced_Analytics_and_Machine_Learning-Chapter_24_Advanced_Analytics_and_Machine_Learning.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Advanced_Analytics_and_Machine_Learning-Chapter_25_Preprocessing_and_Feature_Engineering.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Advanced_Analytics_and_Machine_Learning-Chapter_25_Preprocessing_and_Feature_Engineering.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Advanced_Analytics_and_Machine_Learning-Chapter_26_Classification.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Advanced_Analytics_and_Machine_Learning-Chapter_26_Classification.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Advanced_Analytics_and_Machine_Learning-Chapter_27_Regression.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Advanced_Analytics_and_Machine_Learning-Chapter_27_Regression.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Advanced_Analytics_and_Machine_Learning-Chapter_28_Recommendation.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Advanced_Analytics_and_Machine_Learning-Chapter_28_Recommendation.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Advanced_Analytics_and_Machine_Learning-Chapter_29_Unsupervised_Learning.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Advanced_Analytics_and_Machine_Learning-Chapter_29_Unsupervised_Learning.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Advanced_Analytics_and_Machine_Learning-Chapter_30_Graph_Analysis.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Advanced_Analytics_and_Machine_Learning-Chapter_30_Graph_Analysis.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Advanced_Analytics_and_Machine_Learning-Chapter_31_Deep_Learning.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Advanced_Analytics_and_Machine_Learning_Chapter_24_Advanced_Analytics_and_Machine_Learning.java  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Ecosystem-Chapter_32_Language_Specifics.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Ecosystem-Chapter_32_Language_Specifics.r  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Ecosystem-Chapter_33_Ecosystem_and_Community.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Low_Level_APIs-Chapter_12_RDD_Basics.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Low_Level_APIs-Chapter_12_RDD_Basics.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Low_Level_APIs-Chapter_13_Advanced_RDDs.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Low_Level_APIs-Chapter_13_Advanced_RDDs.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Low_Level_APIs-Chapter_14_Distributed_Variables.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Low_Level_APIs-Chapter_14_Distributed_Variables.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Production_Applications-Chapter_15_How_Spark_Runs_on_a_Cluster.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Production_Applications-Chapter_15_How_Spark_Runs_on_a_Cluster.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Production_Applications-Chapter_16_Spark_Applications.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Production_Applications-Chapter_16_Spark_Applications.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Production_Applications-Chapter_17_Deploying_Spark.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Production_Applications-Chapter_18_Monitoring_and_Debugging.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Production_Applications-Chapter_18_Monitoring_and_Debugging.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Production_Applications-Chapter_19_Performance_Tuning.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Streaming-Chapter_21_Structured_Streaming_Basics.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Streaming-Chapter_21_Structured_Streaming_Basics.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Streaming-Chapter_21_Structured_Streaming_Basics.sql  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Streaming-Chapter_22_Event-Time_and_Stateful_Processing.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Streaming-Chapter_22_Event-Time_and_Stateful_Processing.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Streaming-Chapter_22_Event-Time_and_Stateful_Processing.sql  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Streaming-Chapter_23_Structured_Streaming_in_Production.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Streaming-Chapter_23_Structured_Streaming_in_Production.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Structured_APIs-Chapter_10_Spark_SQL.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Structured_APIs-Chapter_10_Spark_SQL.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Structured_APIs-Chapter_10_Spark_SQL.sql  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Structured_APIs-Chapter_11_Datasets.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Structured_APIs-Chapter_4_Structured_API_Overview.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Structured_APIs-Chapter_4_Structured_API_Overview.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Structured_APIs-Chapter_5_Basic_Structured_Operations.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Structured_APIs-Chapter_5_Basic_Structured_Operations.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Structured_APIs-Chapter_5_Basic_Structured_Operations.sql  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Structured_APIs-Chapter_6_Working_with_Different_Types_of_Data.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Structured_APIs-Chapter_6_Working_with_Different_Types_of_Data.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Structured_APIs-Chapter_6_Working_with_Different_Types_of_Data.sql  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Structured_APIs-Chapter_7_Aggregations.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Structured_APIs-Chapter_7_Aggregations.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Structured_APIs-Chapter_7_Aggregations.sql  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Structured_APIs-Chapter_8_Joins.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Structured_APIs-Chapter_8_Joins.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Structured_APIs-Chapter_8_Joins.sql  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Structured_APIs-Chapter_9_Data_Sources.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Structured_APIs-Chapter_9_Data_Sources.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/code/Structured_APIs_Chapter_4_Structured_API_Overview.java  \n   creating: /tmp/Spark-The-Definitive-Guide-master/data/\n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/README.md  \n   creating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/\n extracting: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/_SUCCESS  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/_committed_730451297822678341  \n extracting: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/_started_730451297822678341  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00000-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00001-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00002-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00003-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00004-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00005-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00006-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00007-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00008-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00009-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00010-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00011-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00012-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00013-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00014-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00015-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00016-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00017-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00018-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00019-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00020-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00021-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00022-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00023-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00024-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00025-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00026-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00027-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00028-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00029-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00030-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00031-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00032-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00033-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00034-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00035-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00036-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00037-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00038-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00039-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00040-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00041-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00042-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00043-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00044-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00045-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00046-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00047-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00048-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00049-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00050-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00051-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00052-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00053-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00054-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00055-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00056-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00057-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00058-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00059-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00060-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00061-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00062-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00063-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00064-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00065-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00066-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00067-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00068-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00069-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00070-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00071-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00072-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00073-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00074-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00075-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00076-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00077-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00078-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/activity-data/part-00079-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json  \n   creating: /tmp/Spark-The-Definitive-Guide-master/data/bike-data/\n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/bike-data/201508_station_data.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/bike-data/201508_trip_data.csv  \n   creating: /tmp/Spark-The-Definitive-Guide-master/data/binary-classification/\n extracting: /tmp/Spark-The-Definitive-Guide-master/data/binary-classification/_SUCCESS  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/binary-classification/part-r-00000-e02e56d5-d522-4b93-a7f2-f2dc1b2fdba9.gz.parquet  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/binary-classification/part-r-00001-e02e56d5-d522-4b93-a7f2-f2dc1b2fdba9.gz.parquet  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/binary-classification/part-r-00002-e02e56d5-d522-4b93-a7f2-f2dc1b2fdba9.gz.parquet  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/binary-classification/part-r-00003-e02e56d5-d522-4b93-a7f2-f2dc1b2fdba9.gz.parquet  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/binary-classification/part-r-00004-e02e56d5-d522-4b93-a7f2-f2dc1b2fdba9.gz.parquet  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/binary-classification/part-r-00005-e02e56d5-d522-4b93-a7f2-f2dc1b2fdba9.gz.parquet  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/binary-classification/part-r-00006-e02e56d5-d522-4b93-a7f2-f2dc1b2fdba9.gz.parquet  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/binary-classification/part-r-00007-e02e56d5-d522-4b93-a7f2-f2dc1b2fdba9.gz.parquet  \n   creating: /tmp/Spark-The-Definitive-Guide-master/data/clustering/\n extracting: /tmp/Spark-The-Definitive-Guide-master/data/clustering/_SUCCESS  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/clustering/part-r-00000-8891f92d-5542-4aec-a830-0d4ff6f5f871.gz.parquet  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/clustering/part-r-00001-8891f92d-5542-4aec-a830-0d4ff6f5f871.gz.parquet  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/clustering/part-r-00002-8891f92d-5542-4aec-a830-0d4ff6f5f871.gz.parquet  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/clustering/part-r-00003-8891f92d-5542-4aec-a830-0d4ff6f5f871.gz.parquet  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/clustering/part-r-00004-8891f92d-5542-4aec-a830-0d4ff6f5f871.gz.parquet  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/clustering/part-r-00005-8891f92d-5542-4aec-a830-0d4ff6f5f871.gz.parquet  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/clustering/part-r-00006-8891f92d-5542-4aec-a830-0d4ff6f5f871.gz.parquet  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/clustering/part-r-00007-8891f92d-5542-4aec-a830-0d4ff6f5f871.gz.parquet  \n   creating: /tmp/Spark-The-Definitive-Guide-master/data/deep-learning-images/\n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/deep-learning-images/LICENSE.txt  \n   creating: /tmp/Spark-The-Definitive-Guide-master/data/deep-learning-images/daisy/\n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/deep-learning-images/daisy/100080576_f52e8ee070_n.jpg  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/deep-learning-images/daisy/10140303196_b88d3d6cec.jpg  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/deep-learning-images/daisy/10172379554_b296050f82_n.jpg  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/deep-learning-images/daisy/10172567486_2748826a8b.jpg  \n  inflatin\n\n*** WARNING: max output size exceeded, skipping output. ***\n\nating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-03-27.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-03-28.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-03-29.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-03-30.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-03-31.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-04-01.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-04-03.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-04-04.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-04-05.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-04-06.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-04-07.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-04-08.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-04-10.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-04-11.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-04-12.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-04-13.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-04-14.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-04-15.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-04-17.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-04-18.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-04-19.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-04-20.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-04-21.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-04-26.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-04-27.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-04-28.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-01.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-03.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-04.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-05.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-06.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-08.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-09.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-10.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-11.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-12.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-13.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-15.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-16.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-17.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-18.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-19.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-20.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-22.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-23.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-24.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-25.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-26.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-27.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-29.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-05-31.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-01.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-02.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-03.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-05.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-06.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-07.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-08.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-09.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-10.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-12.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-13.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-14.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-15.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-16.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-17.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-19.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-20.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-21.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-22.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-23.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-24.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-26.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-27.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-28.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-29.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-06-30.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-01.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-03.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-04.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-05.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-06.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-07.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-08.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-10.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-11.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-12.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-13.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-14.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-15.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-17.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-18.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-19.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-20.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-21.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-22.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-24.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-25.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-26.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-27.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-28.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-29.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-07-31.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-01.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-02.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-03.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-04.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-05.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-07.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-08.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-09.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-10.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-11.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-12.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-14.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-15.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-16.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-17.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-18.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-19.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-21.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-22.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-23.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-24.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-25.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-26.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-28.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-30.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-08-31.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-01.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-02.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-04.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-05.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-06.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-07.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-08.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-09.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-11.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-12.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-13.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-14.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-15.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-16.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-18.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-19.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-20.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-21.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-22.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-23.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-25.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-26.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-27.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-28.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-29.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-09-30.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-02.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-03.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-04.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-05.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-06.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-07.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-09.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-10.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-11.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-12.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-13.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-14.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-16.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-17.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-18.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-19.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-20.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-21.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-23.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-24.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-25.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-26.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-27.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-28.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-30.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-10-31.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-01.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-02.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-03.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-04.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-06.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-07.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-08.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-09.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-10.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-11.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-13.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-14.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-15.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-16.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-17.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-18.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-20.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-21.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-22.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-23.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-24.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-25.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-27.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-28.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-29.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-11-30.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-12-01.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-12-02.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-12-04.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-12-05.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-12-06.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-12-07.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-12-08.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2011-12-09.csv  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/sample_libsvm_data.txt  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/sample_movielens_ratings.txt  \n   creating: /tmp/Spark-The-Definitive-Guide-master/data/simple-ml-integers/\n extracting: /tmp/Spark-The-Definitive-Guide-master/data/simple-ml-integers/_SUCCESS  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/simple-ml-integers/part-00000-ce2a44c8-feb4-4369-a2c3-4bf2f0e63b07-c000.gz.parquet  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/simple-ml-integers/part-00001-ce2a44c8-feb4-4369-a2c3-4bf2f0e63b07-c000.gz.parquet  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/simple-ml-integers/part-00002-ce2a44c8-feb4-4369-a2c3-4bf2f0e63b07-c000.gz.parquet  \n   creating: /tmp/Spark-The-Definitive-Guide-master/data/simple-ml-scaling/\n extracting: /tmp/Spark-The-Definitive-Guide-master/data/simple-ml-scaling/_SUCCESS  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/simple-ml-scaling/part-00000-cd03406a-cc9b-42b0-9299-1e259fdd9382-c000.gz.parquet  \n   creating: /tmp/Spark-The-Definitive-Guide-master/data/simple-ml/\n extracting: /tmp/Spark-The-Definitive-Guide-master/data/simple-ml/_SUCCESS  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/data/simple-ml/part-r-00000-f5c243b9-a015-4a3b-a4a8-eca00f80f04c.json  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/license.md  \n   creating: /tmp/Spark-The-Definitive-Guide-master/project-templates/\n  inflating: /tmp/Spark-The-Definitive-Guide-master/project-templates/.gitignore  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/project-templates/README.md  \n   creating: /tmp/Spark-The-Definitive-Guide-master/project-templates/example-data/\n  inflating: /tmp/Spark-The-Definitive-Guide-master/project-templates/example-data/data.json  \n   creating: /tmp/Spark-The-Definitive-Guide-master/project-templates/java/\n extracting: /tmp/Spark-The-Definitive-Guide-master/project-templates/java/.gitignore  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/project-templates/java/example.iml  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/project-templates/java/pom.xml  \n   creating: /tmp/Spark-The-Definitive-Guide-master/project-templates/java/src/\n   creating: /tmp/Spark-The-Definitive-Guide-master/project-templates/java/src/main/\n   creating: /tmp/Spark-The-Definitive-Guide-master/project-templates/java/src/main/java/\n   creating: /tmp/Spark-The-Definitive-Guide-master/project-templates/java/src/main/java/com/\n   creating: /tmp/Spark-The-Definitive-Guide-master/project-templates/java/src/main/java/com/databricks/\n   creating: /tmp/Spark-The-Definitive-Guide-master/project-templates/java/src/main/java/com/databricks/example/\n  inflating: /tmp/Spark-The-Definitive-Guide-master/project-templates/java/src/main/java/com/databricks/example/SimpleExample.java  \n   creating: /tmp/Spark-The-Definitive-Guide-master/project-templates/python/\n   creating: /tmp/Spark-The-Definitive-Guide-master/project-templates/python/pyspark_template/\n extracting: /tmp/Spark-The-Definitive-Guide-master/project-templates/python/pyspark_template/__init__.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/project-templates/python/pyspark_template/main.py  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/project-templates/python/setup.py  \n   creating: /tmp/Spark-The-Definitive-Guide-master/project-templates/scala/\n  inflating: /tmp/Spark-The-Definitive-Guide-master/project-templates/scala/.gitignore  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/project-templates/scala/build.sbt  \n   creating: /tmp/Spark-The-Definitive-Guide-master/project-templates/scala/src/\n   creating: /tmp/Spark-The-Definitive-Guide-master/project-templates/scala/src/main/\n   creating: /tmp/Spark-The-Definitive-Guide-master/project-templates/scala/src/main/resources/\n  inflating: /tmp/Spark-The-Definitive-Guide-master/project-templates/scala/src/main/resources/log4j.properties  \n   creating: /tmp/Spark-The-Definitive-Guide-master/project-templates/scala/src/main/scala/\n  inflating: /tmp/Spark-The-Definitive-Guide-master/project-templates/scala/src/main/scala/DataFrameExample.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/project-templates/scala/src/main/scala/DatasetExample.scala  \n   creating: /tmp/Spark-The-Definitive-Guide-master/project-templates/scala/src/test/\n   creating: /tmp/Spark-The-Definitive-Guide-master/project-templates/scala/src/test/resources/\n  inflating: /tmp/Spark-The-Definitive-Guide-master/project-templates/scala/src/test/resources/log4j.properties  \n   creating: /tmp/Spark-The-Definitive-Guide-master/project-templates/scala/src/test/scala/\n  inflating: /tmp/Spark-The-Definitive-Guide-master/project-templates/scala/src/test/scala/DataFrameExampleTest.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/project-templates/scala/src/test/scala/DatasetExampleTest.scala  \n  inflating: /tmp/Spark-The-Definitive-Guide-master/project-templates/scala/src/test/scala/TestBase.scala  \n"
     ]
    }
   ],
   "source": [
    "%sh unzip /tmp/Spark_The_Definitive_Guide_master.zip -d /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b4beb4e-4dfe-41fa-b425-53b5173ebf24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- InvoiceDate: timestamp (nullable = true)\n |-- UnitPrice: double (nullable = true)\n |-- CustomerID: double (nullable = true)\n |-- Country: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(\"file:///tmp/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2010-12-01.csv\")\n",
    "df.printSchema()\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00a037b3-072a-49ea-91cb-71a3383cc562",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- 5: integer (nullable = false)\n |-- five: string (nullable = false)\n |-- 5.0: double (nullable = false)\n\n"
     ]
    }
   ],
   "source": [
    "# Converting to Spark Types\n",
    "# convert native types to Spark types\n",
    "# one such function is lit: This function converts a type in another language to its correspnding Spark representation.\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "df.select(lit(5), lit(\"five\"), lit(5.0)).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ce8bbf2-a56e-4249-9c00-da3f6897e9af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------+\n|InvoiceNo|Description                  |\n+---------+-----------------------------+\n|536366   |HAND WARMER UNION JACK       |\n|536366   |HAND WARMER RED POLKA DOT    |\n|536367   |ASSORTED COLOUR BIRD ORNAMENT|\n|536367   |POPPY'S PLAYHOUSE BEDROOM    |\n|536367   |POPPY'S PLAYHOUSE KITCHEN    |\n+---------+-----------------------------+\nonly showing top 5 rows\n\n+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|Description                        |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |\n+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER |6       |2010-12-01 08:26:00|2.55     |17850.0   |United Kingdom|\n|536365   |71053    |WHITE METAL LANTERN                |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n|536365   |84406B   |CREAM CUPID HEARTS COAT HANGER     |8       |2010-12-01 08:26:00|2.75     |17850.0   |United Kingdom|\n|536365   |84029G   |KNITTED UNION FLAG HOT WATER BOTTLE|6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n|536365   |84029E   |RED WOOLLY HOTTIE WHITE HEART.     |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Working with Booleans\n",
    "from pyspark.sql.functions import col\n",
    "df.where(col(\"InvoiceNo\") != 536365)\\\n",
    ".select(\"InvoiceNo\", \"Description\")\\\n",
    ".show(5, False)\n",
    "\n",
    "df.where(\"InvoiceNo = 536365\") \\\n",
    ".show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51a0932b-602d-48f3-b825-f4c6f335c917",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|\n|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|\n+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# In Spark, you should always chain together and filters as a sequential filter.\n",
    "# The reason for this is that even if Boolean statements are expressed serially (one after the other), Spark will flatten all of these filters into one statement and perform the filter at the same time, creating the and statement for us\n",
    "# or statements need to be specified in the same statement\n",
    "\n",
    "from pyspark.sql.functions import instr\n",
    "priceFilter = col(\"UnitPrice\") > 600\n",
    "descripFilter = instr(df.Description, \"POSTAGE\") >= 1\n",
    "df.where(df.StockCode.isin(\"DOT\")).where(priceFilter | descripFilter).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83acd0e4-bee9-44b9-8c2b-6b26186088a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n|unitPrice|isExpensive|\n+---------+-----------+\n|   569.77|       true|\n|   607.49|       true|\n+---------+-----------+\n\n+--------------+---------+\n|   Description|UnitPrice|\n+--------------+---------+\n|DOTCOM POSTAGE|   569.77|\n|DOTCOM POSTAGE|   607.49|\n+--------------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# Boolean expressions are not just reserved to filters. To filter a DataFrame, you can also just specify a Boolean column:\n",
    "from pyspark.sql.functions import instr, col\n",
    "DOTCodeFilter = col(\"StockCode\") == \"DOT\"\n",
    "priceFilter = col(\"UnitPrice\") > 600\n",
    "descripFilter = instr(col(\"Description\"), \"POSTAGE\") >= 1\n",
    "df.withColumn(\"isExpensive\", DOTCodeFilter & (priceFilter | descripFilter))\\\n",
    ".where(\"isExpensive\")\\\n",
    ".select(\"unitPrice\", \"isExpensive\").show(5)\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "df.withColumn(\"isExpensive\", expr(\"NOT UnitPrice <= 250\"))\\\n",
    ".where(\"isExpensive\")\\\n",
    ".select(\"Description\", \"UnitPrice\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e486bec-c854-4569-b94e-3972d68a7148",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+----------------+\n|(value = foo)|(value <=> foo)|(value <=> NULL)|\n+-------------+---------------+----------------+\n|         true|           true|           false|\n|         null|          false|            true|\n+-------------+---------------+----------------+\n\n0\n1\n+---+-----+\n| id|value|\n+---+-----+\n|  1|  NaN|\n|  2| 42.0|\n|  3| null|\n+---+-----+\n\n+----------------+---------------+----------------+\n|(value <=> NULL)|(value <=> NaN)|(value <=> 42.0)|\n+----------------+---------------+----------------+\n|           false|           true|           false|\n|           false|          false|            true|\n|            true|          false|           false|\n+----------------+---------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# One “gotcha” that can come up is if you’re working with null data when creating Boolean expressions.\n",
    "# If there is a null in your data, you’ll need to treat things a bit differently. Here’s how you can ensure\n",
    "# that you perform a null-safe equivalence test:\n",
    "\n",
    "from pyspark.sql import Row\n",
    "df1 = spark.createDataFrame([\n",
    "    Row(id=1, value='foo'),\n",
    "    Row(id=2, value=None)\n",
    "])\n",
    "df1.select(\n",
    "    df1['value'] == 'foo',\n",
    "    df1['value'].eqNullSafe('foo'),\n",
    "    df1['value'].eqNullSafe(None)\n",
    ").show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df2 = spark.createDataFrame([\n",
    "    Row(value = 'bar'),\n",
    "    Row(value = None)\n",
    "])\n",
    "print(df1.join(df2, df1[\"value\"] == df2[\"value\"]).count())\n",
    "\n",
    "print(df1.join(df2, df1[\"value\"].eqNullSafe(df2[\"value\"])).count())\n",
    "\n",
    "df2 = spark.createDataFrame([\n",
    "    Row(id=1, value=float('NaN')),\n",
    "    Row(id=2, value=42.0),\n",
    "    Row(id=3, value=None)\n",
    "])\n",
    "df2.show()\n",
    "df2.select(\n",
    "    df2['value'].eqNullSafe(None),\n",
    "    df2['value'].eqNullSafe(float('NaN')),\n",
    "    df2['value'].eqNullSafe(42.0)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d162ffd8-3aa3-4374-b28a-eae3aeee7a92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n|CustomerId|      realQuantity|\n+----------+------------------+\n|   17850.0|239.08999999999997|\n|   17850.0|          418.7156|\n+----------+------------------+\nonly showing top 2 rows\n\n+----------+------------------+\n|CustomerId|      realQuantity|\n+----------+------------------+\n|   17850.0|239.08999999999997|\n|   17850.0|          418.7156|\n+----------+------------------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Working with Numbers\n",
    "from pyspark.sql.functions import expr, pow\n",
    "fabricatedQuantity = pow(col(\"Quantity\") * col(\"UnitPrice\"), 2) + 5\n",
    "df.select(expr(\"CustomerId\"), fabricatedQuantity.alias(\"realQuantity\")).show(2)\n",
    "\n",
    "# Same Can Be Done Using SQL Expression\n",
    "df.selectExpr(\n",
    "\"CustomerId\",\n",
    "\"(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "120550ee-90f4-48a0-b2bb-11ef2ec92ffc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n|round(2.5, 0)|bround(2.5, 0)|\n+-------------+--------------+\n|          3.0|           2.0|\n|          3.0|           2.0|\n+-------------+--------------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "# the round function rounds up if you’re exactly in between two numbers. You can round down by using the bround:\n",
    "from pyspark.sql.functions import lit, round, bround\n",
    "df.select(round(lit(\"2.5\")), bround(lit(\"2.5\"))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aec447fc-f7e9-4dcb-8a83-77bd608d8df9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n|corr(Quantity, UnitPrice)|\n+-------------------------+\n|     -0.04112314436835551|\n+-------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# numerical task is to compute the correlation of two columns. For example, we can see\n",
    "# the Pearson correlation coefficient for two columns to see if cheaper things are typically bought\n",
    "# in greater quantities. We can do this through a function as well as through the DataFrame\n",
    "# statistic methods:\n",
    "\n",
    "from pyspark.sql.functions import corr\n",
    "df.stat.corr(\"Quantity\", \"UnitPrice\")\n",
    "df.select(corr(\"Quantity\", \"UnitPrice\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0494ea7-53aa-4f78-ac54-32f8052bd279",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n|summary|        InvoiceNo|         StockCode|         Description|          Quantity|         UnitPrice|        CustomerID|       Country|\n+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n|  count|             3108|              3108|                3098|              3108|              3108|              1968|          3108|\n|   mean| 536516.684944841|27834.304044117645|                null| 8.627413127413128| 4.151946589446603|15661.388719512195|          null|\n| stddev|72.89447869788873|17407.897548583845|                null|26.371821677029203|15.638659854603892|1854.4496996893627|          null|\n|    min|           536365|             10002| 4 PURPLE FLOCK D...|               -24|               0.0|           12431.0|     Australia|\n|    max|          C536548|              POST|ZINC WILLIE WINKI...|               600|            607.49|           18229.0|United Kingdom|\n+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Another common task is to compute summary statistics for a column or set of columns. We can\n",
    "# use the describe method to achieve exactly this. This will take all numeric columns and\n",
    "# calculate the count, mean, standard deviation, min, and max\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55a8dca1-db48-44ac-8d3c-168d3c5d6c77",
     "showTitle": true,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n|monotonically_increasing_id()|\n+-----------------------------+\n|                            0|\n|                            1|\n+-----------------------------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "# As a last note, we can also add a unique ID to each row by using the function\n",
    "# monotonically_increasing_id. This function generates a unique value for each row, starting\n",
    "# with 0:\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "df.select(monotonically_increasing_id()).show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "340984d2-be14-42ed-81ee-389087d33612",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|initcap(Description)|\n+--------------------+\n|White Hanging Hea...|\n| White Metal Lantern|\n|Cream Cupid Heart...|\n|Knitted Union Fla...|\n|Red Woolly Hottie...|\n+--------------------+\nonly showing top 5 rows\n\n+--------------------+--------------------+-------------------------+\n|         Description|  lower(Description)|upper(lower(Description))|\n+--------------------+--------------------+-------------------------+\n|WHITE HANGING HEA...|white hanging hea...|     WHITE HANGING HEA...|\n| WHITE METAL LANTERN| white metal lantern|      WHITE METAL LANTERN|\n|CREAM CUPID HEART...|cream cupid heart...|     CREAM CUPID HEART...|\n|KNITTED UNION FLA...|knitted union fla...|     KNITTED UNION FLA...|\n|RED WOOLLY HOTTIE...|red woolly hottie...|     RED WOOLLY HOTTIE...|\n+--------------------+--------------------+-------------------------+\nonly showing top 5 rows\n\n+------+------+-----+---+----------+\n| ltrim| rtrim| trim| lp|        rp|\n+------+------+-----+---+----------+\n|HELLO | HELLO|HELLO| HE|HELLO     |\n|HELLO | HELLO|HELLO| HE|HELLO     |\n+------+------+-----+---+----------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Working with Strings\n",
    "# String Manipulation Functions\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import initcap \n",
    "df.select(initcap(col(\"Description\"))).show(5) # Capitlize First Char Of Word\n",
    "\n",
    "from pyspark.sql.functions import lower, upper\n",
    "df.select(col(\"Description\"),\n",
    "lower(col(\"Description\")),\n",
    "upper(lower(col(\"Description\")))).show(5) \n",
    "\n",
    "from pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trim\n",
    "df.select(\n",
    "ltrim(lit(\" HELLO \")).alias(\"ltrim\"),\n",
    "rtrim(lit(\" HELLO \")).alias(\"rtrim\"),\n",
    "trim(lit(\" HELLO \")).alias(\"trim\"),\n",
    "lpad(lit(\"HELLLLLLL\"), 2, \" \").alias(\"lp\"), # Add Remaining Number Of Values TO make String Len given\n",
    "rpad(lit(\"HELLO\"), 10, \" \").alias(\"rp\")).show(2)\n",
    "# Note that if lpad or rpad takes a number less than the length of the string, it will always remove\n",
    "# values from the right side of the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "729ede1f-8805-4e5c-9274-d73ed82cc4a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+--------------------------------------------------------+\n|Description                        |REGEXP_EXAMPLE                                          |\n+-----------------------------------+--------------------------------------------------------+\n|WHITE HANGING HEART T-LIGHT HOLDER |COLOR REPLACEMENT HANGING HEART T-LIGHT HOLDER          |\n|WHITE METAL LANTERN                |COLOR REPLACEMENT METAL LANTERN                         |\n|CREAM CUPID HEARTS COAT HANGER     |CREAM CUPID HEARTS COAT HANGER                          |\n|KNITTED UNION FLAG HOT WATER BOTTLE|KNITTED UNION FLAG HOT WATER BOTTLE                     |\n|RED WOOLLY HOTTIE WHITE HEART.     |COLOR REPLACEMENT WOOLLY HOTTIE COLOR REPLACEMENT HEART.|\n+-----------------------------------+--------------------------------------------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Regex Expression\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "regex_string = \"BLACK|WHITE|RED|GREEN|BLUE\"\n",
    "\n",
    "df.select(df.Description, regexp_replace(df.Description, regex_string, \"COLOR REPLACEMENT\").alias(\"REGEXP_EXAMPLE\")).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce1baee7-f5b5-449d-bc9e-3c4651140dd0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------+\n|translate(Description, LEET, 1347)|Description                       |\n+----------------------------------+----------------------------------+\n|WHI73 HANGING H3AR7 7-1IGH7 HO1D3R|WHITE HANGING HEART T-LIGHT HOLDER|\n|WHI73 M37A1 1AN73RN               |WHITE METAL LANTERN               |\n+----------------------------------+----------------------------------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import translate\n",
    "df.select(translate(col(\"Description\"), \"LEET\", \"1347\"),col(\"Description\"))\\\n",
    ".show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe346170-d095-4952-85f7-d4ce5c59963d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------+\n|Description                        |result|\n+-----------------------------------+------+\n|WHITE HANGING HEART T-LIGHT HOLDER |WHITE |\n|WHITE METAL LANTERN                |WHITE |\n|CREAM CUPID HEARTS COAT HANGER     |      |\n|KNITTED UNION FLAG HOT WATER BOTTLE|      |\n|RED WOOLLY HOTTIE WHITE HEART.     |RED   |\n+-----------------------------------+------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "regex_string = \"(BLACK|WHITE|RED|GREEN|BLUE)\"\n",
    "\n",
    "df.select(df.Description, regexp_extract(df.Description, regex_string, 1).alias(\"result\")).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfe606c1-6482-40ec-af10-dc04528f8b31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n|Description                       |\n+----------------------------------+\n|WHITE HANGING HEART T-LIGHT HOLDER|\n|WHITE METAL LANTERN               |\n|RED WOOLLY HOTTIE WHITE HEART.    |\n+----------------------------------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import instr\n",
    "containsBlack = instr(col(\"Description\"), \"BLACK\") >= 1\n",
    "containsWhite = instr(col(\"Description\"), \"WHITE\") >= 1\n",
    "df.withColumn(\"hasSimpleColor\", containsBlack | containsWhite)\\\n",
    ".where(\"hasSimpleColor\")\\\n",
    ".select(\"Description\").show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32e0dff3-21de-4507-9500-4b090eb7b972",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Column<'CAST(locate(BLACK, Description, 1) AS BOOLEAN) AS is_black'>, Column<'CAST(locate(WHITE, Description, 1) AS BOOLEAN) AS is_white'>, Column<'CAST(locate(RED, Description, 1) AS BOOLEAN) AS is_red'>, Column<'CAST(locate(GREEN, Description, 1) AS BOOLEAN) AS is_green'>, Column<'CAST(locate(BLUE, Description, 1) AS BOOLEAN) AS is_blue'>]\n[Column<'CAST(locate(BLACK, Description, 1) AS BOOLEAN) AS is_black'>, Column<'CAST(locate(WHITE, Description, 1) AS BOOLEAN) AS is_white'>, Column<'CAST(locate(RED, Description, 1) AS BOOLEAN) AS is_red'>, Column<'CAST(locate(GREEN, Description, 1) AS BOOLEAN) AS is_green'>, Column<'CAST(locate(BLUE, Description, 1) AS BOOLEAN) AS is_blue'>, Column<'unresolvedstar()'>]\n+--------+--------+------+--------+-------+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|is_black|is_white|is_red|is_green|is_blue|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+--------+--------+------+--------+-------+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   false|    true| false|   false|  false|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n|   false|    true| false|   false|  false|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n|   false|   false| false|   false|  false|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n|   false|   false| false|   false|  false|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n|   false|    true|  true|   false|  false|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n|   false|   false| false|   false|  false|   536365|    22752|SET 7 BABUSHKA NE...|       2|2010-12-01 08:26:00|     7.65|   17850.0|United Kingdom|\n|   false|   false| false|   false|  false|   536365|    21730|GLASS STAR FROSTE...|       6|2010-12-01 08:26:00|     4.25|   17850.0|United Kingdom|\n|   false|   false| false|   false|  false|   536366|    22633|HAND WARMER UNION...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n|   false|   false|  true|   false|  false|   536366|    22632|HAND WARMER RED P...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n|   false|   false| false|   false|  false|   536367|    84879|ASSORTED COLOUR B...|      32|2010-12-01 08:34:00|     1.69|   13047.0|United Kingdom|\n|   false|   false| false|   false|  false|   536367|    22745|POPPY'S PLAYHOUSE...|       6|2010-12-01 08:34:00|      2.1|   13047.0|United Kingdom|\n|   false|   false| false|   false|  false|   536367|    22748|POPPY'S PLAYHOUSE...|       6|2010-12-01 08:34:00|      2.1|   13047.0|United Kingdom|\n|   false|   false| false|   false|  false|   536367|    22749|FELTCRAFT PRINCES...|       8|2010-12-01 08:34:00|     3.75|   13047.0|United Kingdom|\n|   false|   false| false|   false|  false|   536367|    22310|IVORY KNITTED MUG...|       6|2010-12-01 08:34:00|     1.65|   13047.0|United Kingdom|\n|   false|   false| false|   false|  false|   536367|    84969|BOX OF 6 ASSORTED...|       6|2010-12-01 08:34:00|     4.25|   13047.0|United Kingdom|\n|   false|   false| false|   false|  false|   536367|    22623|BOX OF VINTAGE JI...|       3|2010-12-01 08:34:00|     4.95|   13047.0|United Kingdom|\n|   false|   false| false|   false|  false|   536367|    22622|BOX OF VINTAGE AL...|       2|2010-12-01 08:34:00|     9.95|   13047.0|United Kingdom|\n|   false|   false| false|   false|  false|   536367|    21754|HOME BUILDING BLO...|       3|2010-12-01 08:34:00|     5.95|   13047.0|United Kingdom|\n|   false|   false| false|   false|  false|   536367|    21755|LOVE BUILDING BLO...|       3|2010-12-01 08:34:00|     5.95|   13047.0|United Kingdom|\n|   false|   false| false|   false|  false|   536367|    21777|RECIPE BOX WITH M...|       4|2010-12-01 08:34:00|     7.95|   13047.0|United Kingdom|\n+--------+--------+------+--------+-------+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 20 rows\n\n+----------------------------------+\n|Description                       |\n+----------------------------------+\n|WHITE HANGING HEART T-LIGHT HOLDER|\n|WHITE METAL LANTERN               |\n|RED WOOLLY HOTTIE WHITE HEART.    |\n+----------------------------------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, locate\n",
    "simpleColors = [\"black\", \"white\", \"red\", \"green\", \"blue\"]\n",
    "def color_locator(column, color_string):\n",
    "    return locate(color_string.upper(), column).cast(\"boolean\").alias(\"is_\" + color_string)\n",
    "\n",
    "selectedColumns = [color_locator(df.Description, c) for c in simpleColors]\n",
    "print (selectedColumns)\n",
    "selectedColumns.append(expr(\"*\")) # has to a be Column type\n",
    "print (selectedColumns)\n",
    "df.select(*selectedColumns).show()\n",
    "df.select(*selectedColumns).where(expr(\"is_white OR is_red\"))\\\n",
    ".select(\"Description\").show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e67be16-79ea-45a6-be28-b274a4e613ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------------------+\n|id |curr_date |curr_timestamp         |\n+---+----------+-----------------------+\n|0  |2024-02-18|2024-02-18 04:37:34.964|\n|1  |2024-02-18|2024-02-18 04:37:34.964|\n|2  |2024-02-18|2024-02-18 04:37:34.964|\n|3  |2024-02-18|2024-02-18 04:37:34.964|\n|4  |2024-02-18|2024-02-18 04:37:34.964|\n|5  |2024-02-18|2024-02-18 04:37:34.964|\n|6  |2024-02-18|2024-02-18 04:37:34.964|\n|7  |2024-02-18|2024-02-18 04:37:34.964|\n|8  |2024-02-18|2024-02-18 04:37:34.964|\n|9  |2024-02-18|2024-02-18 04:37:34.964|\n+---+----------+-----------------------+\n\n+---+----------+----------+----------+\n|id |curr_date |5add      |5sub      |\n+---+----------+----------+----------+\n|0  |2024-02-18|2024-02-23|2024-02-13|\n|1  |2024-02-18|2024-02-23|2024-02-13|\n|2  |2024-02-18|2024-02-23|2024-02-13|\n|3  |2024-02-18|2024-02-23|2024-02-13|\n|4  |2024-02-18|2024-02-23|2024-02-13|\n|5  |2024-02-18|2024-02-23|2024-02-13|\n|6  |2024-02-18|2024-02-23|2024-02-13|\n|7  |2024-02-18|2024-02-23|2024-02-13|\n|8  |2024-02-18|2024-02-23|2024-02-13|\n|9  |2024-02-18|2024-02-23|2024-02-13|\n+---+----------+----------+----------+\n\n+---+----------+----------+---------+\n|id |today     |5add      |date_diff|\n+---+----------+----------+---------+\n|0  |2024-02-18|2024-02-23|-5       |\n|1  |2024-02-18|2024-02-23|-5       |\n|2  |2024-02-18|2024-02-23|-5       |\n|3  |2024-02-18|2024-02-23|-5       |\n|4  |2024-02-18|2024-02-23|-5       |\n|5  |2024-02-18|2024-02-23|-5       |\n|6  |2024-02-18|2024-02-23|-5       |\n|7  |2024-02-18|2024-02-23|-5       |\n|8  |2024-02-18|2024-02-23|-5       |\n|9  |2024-02-18|2024-02-23|-5       |\n+---+----------+----------+---------+\n\n+---+----------+----------+----------+----------------+----------------+\n|id |today     |60add     |55add     |months_between60|months_between55|\n+---+----------+----------+----------+----------------+----------------+\n|0  |2024-02-18|2024-04-18|2024-04-13|-2.0            |-1.83870968     |\n|1  |2024-02-18|2024-04-18|2024-04-13|-2.0            |-1.83870968     |\n|2  |2024-02-18|2024-04-18|2024-04-13|-2.0            |-1.83870968     |\n|3  |2024-02-18|2024-04-18|2024-04-13|-2.0            |-1.83870968     |\n|4  |2024-02-18|2024-04-18|2024-04-13|-2.0            |-1.83870968     |\n|5  |2024-02-18|2024-04-18|2024-04-13|-2.0            |-1.83870968     |\n|6  |2024-02-18|2024-04-18|2024-04-13|-2.0            |-1.83870968     |\n|7  |2024-02-18|2024-04-18|2024-04-13|-2.0            |-1.83870968     |\n|8  |2024-02-18|2024-04-18|2024-04-13|-2.0            |-1.83870968     |\n|9  |2024-02-18|2024-04-18|2024-04-13|-2.0            |-1.83870968     |\n+---+----------+----------+----------+----------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Spark’s TimestampType class supports only second-level precision, which means that if you’re going to be working with milliseconds or microseconds, you’ll need to work around this problem by potentially operating on them as longs. Any more precision when coercing to a TimestampType will be removed.\n",
    "\n",
    "# Spark can be a bit particular about what format you have at any given point in time. It’s important to be explicit when parsing or converting to ensure that there are no issues in doing so. At the end of the day, Spark is working with Java dates and timestamps and therefore conforms to those standards\n",
    "\n",
    "# By Default TimeStamp will be shown in UTC\n",
    "from pyspark.sql.functions import current_date, current_timestamp\n",
    "spark.range(10).withColumn(\"curr_date\", current_date()).withColumn(\"curr_timestamp\", current_timestamp()).show(truncate=False)\n",
    "\n",
    "from pyspark.sql.functions import col, date_add, date_sub\n",
    "spark.range(10).withColumn('curr_date', current_date())\\\n",
    "    .withColumn(\"5add\", date_add(col(\"curr_date\"),5))\\\n",
    "    .withColumn(\"5sub\", date_sub(col(\"curr_date\"), 5))\\\n",
    "    .show(truncate=False)\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import datediff\n",
    "spark.range(10)\\\n",
    "    .withColumn('today', current_date())\\\n",
    "    .withColumn('5add', date_add(col('today'), 5))\\\n",
    "    .withColumn('date_diff', datediff(col('today'), col('5add')))\\\n",
    "    .show(truncate=False)\n",
    "\n",
    "from pyspark.sql.functions import months_between\n",
    "spark.range(10)\\\n",
    "    .withColumn('today', current_date())\\\n",
    "    .withColumn('60add', date_add(col('today'), 60))\\\n",
    "    .withColumn('55add', date_add(col('today'), 55))\\\n",
    "    .withColumn('months_between60', months_between(col('today'), col('60add')))\\\n",
    "    .withColumn('months_between55', months_between(col('today'), col('55add')))\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61de5ae2-6e9c-4ebb-96cb-2e452d3e205d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n|to_date(date)|\n+-------------+\n|   2017-01-01|\n+-------------+\nonly showing top 1 row\n\nroot\n |-- to_date(date): date (nullable = true)\n\n+-------------------+-------------------+\n|to_date(2016-20-12)|to_date(2017-12-11)|\n+-------------------+-------------------+\n|               null|         2017-12-11|\n+-------------------+-------------------+\nonly showing top 1 row\n\n+----------+----------+\n|      date|     date2|\n+----------+----------+\n|2017-11-12|2017-12-20|\n+----------+----------+\n\n+------------------------------+\n|to_timestamp(date, yyyy-dd-MM)|\n+------------------------------+\n|           2017-11-12 00:00:00|\n+------------------------------+\n\n+----------+----------+\n|      date|     date2|\n+----------+----------+\n|2017-11-12|2017-12-20|\n+----------+----------+\n\n+----+-----+\n|date|date2|\n+----+-----+\n+----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, lit, col\n",
    "spark.range(5).withColumn(\"date\", lit(\"2017-01-01\"))\\\n",
    ".select(to_date(col(\"date\"))).show(1)\n",
    "spark.range(5).withColumn(\"date\", lit(\"2017-01-01\"))\\\n",
    ".select(to_date(col(\"date\"))).printSchema()\n",
    "\n",
    "# Spark will not throw an error if it cannot parse the date; rather, it will just return null. This can be a bit tricky in larger pipelines because you might be expecting your data in one format and getting it in another.\n",
    "\n",
    "spark.range(5).select(to_date(lit(\"2016-20-12\")),to_date(lit(\"2017-12-11\"))).show(1)\n",
    "\n",
    "# We find this to be an especially tricky situation for bugs because some dates might match the correct format, whereas others do not. In the previous example, notice how the second date appears as Decembers 11th instead of the correct day, November 12th. Spark doesn’t throw an error because it cannot know whether the days are mixed up or that specific row is incorrect.\n",
    "# Step by step, and come up with a robust way to avoid these issues entirely.\n",
    "# The first step is to remember that we need to specify our date format according to the Java SimpleDateFormat standard.\n",
    "\n",
    "from pyspark.sql.functions import to_date\n",
    "dateFormat = \"yyyy-dd-MM\"\n",
    "cleanDateDF = spark.range(1).select(\n",
    "to_date(lit(\"2017-12-11\"), dateFormat).alias(\"date\"),\n",
    "to_date(lit(\"2017-20-12\"), dateFormat).alias(\"date2\"))\n",
    "cleanDateDF.show()\n",
    "\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "cleanDateDF.select(to_timestamp(col(\"date\"), dateFormat)).show()\n",
    "\n",
    "# After we have our date or timestamp in the correct format and type, comparing between them is actually quite easy. We just need to be sure to either use a date/timestamp type or specify our string according to the right format of yyyy-MM-dd if we’re comparing a date:\n",
    "\n",
    "cleanDateDF.filter(col(\"date2\") > lit(\"2017-12-12\")).show()\n",
    "\n",
    "# One minor point is that we can also set this as a string, which Spark parses to a literal:\n",
    "cleanDateDF.filter(col(\"date2\") > \"'2017-12-12'\").show()\n",
    "\n",
    "# Implicit type casting is an easy way to shoot yourself in the foot, especially when dealing with null values or dates in different timezones or formats. We recommend that you parse them explicitly instead of relying on implicit conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d128704c-a1a6-423d-a1a6-0884933dfc19",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n|coalesce(Description, CustomerId)|\n+---------------------------------+\n|             WHITE HANGING HEA...|\n|              WHITE METAL LANTERN|\n|             CREAM CUPID HEART...|\n|             KNITTED UNION FLA...|\n|             RED WOOLLY HOTTIE...|\n|             SET 7 BABUSHKA NE...|\n|             GLASS STAR FROSTE...|\n|             HAND WARMER UNION...|\n|             HAND WARMER RED P...|\n|             ASSORTED COLOUR B...|\n|             POPPY'S PLAYHOUSE...|\n|             POPPY'S PLAYHOUSE...|\n|             FELTCRAFT PRINCES...|\n|             IVORY KNITTED MUG...|\n|             BOX OF 6 ASSORTED...|\n|             BOX OF VINTAGE JI...|\n|             BOX OF VINTAGE AL...|\n|             HOME BUILDING BLO...|\n|             LOVE BUILDING BLO...|\n|             RECIPE BOX WITH M...|\n+---------------------------------+\nonly showing top 20 rows\n\n+--------------------------+------------------------------------+\n|ifnull(NULL, return_value)|ifnull(ifnull_default, return_value)|\n+--------------------------+------------------------------------+\n|              return_value|                      ifnull_default|\n+--------------------------+------------------------------------+\n\n+------------------+--------------------+-------------------+-----------------------+\n|nullif(NULL, NULL)|nullif(value, value)|nullif(NULL, value)|nullif(notvalue, value)|\n+------------------+--------------------+-------------------+-----------------------+\n|              null|                null|               null|               notvalue|\n+------------------+--------------------+-------------------+-----------------------+\n\n+-----------------------+------------------------------+\n|nvl(NULL, second_value)|nvl(first_value, second_value)|\n+-----------------------+------------------------------+\n|           second_value|                   first_value|\n+-----------------------+------------------------------+\n\n+---------------------------------------+----------------------------------------------+\n|nvl2(NULL, second_value, second_value2)|nvl2(first_value, second_value, second_value2)|\n+---------------------------------------+----------------------------------------------+\n|                          second_value2|                                  second_value|\n+---------------------------------------+----------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Working with Nulls in Data\n",
    "# As a best practice, you should always use nulls to represent missing or empty data in your DataFrames\n",
    "# Spark can optimize working with null values more than it can if you use empty strings or other values\n",
    "# The primary way of interacting with null values, at DataFrame scale, is to use the .na subpackage on a DataFrame\n",
    "\n",
    "# Nulls are a challenging part of all programming, and Spark is no exception. In our opinion, being\n",
    "# explicit is always better than being implicit when handling null values. For instance, in this part of the\n",
    "# book, we saw how we can define columns as having null types. However, this comes with a catch.\n",
    "# When we declare a column as not having a null time, that is not actually enforced. To reiterate, when\n",
    "# you define a schema in which all columns are declared to not have null values, Spark will not enforce\n",
    "# that and will happily let null values into that column. The nullable signal is simply to help Spark SQL\n",
    "# optimize for handling that column. If you have null values in columns that should not have null values,\n",
    "# you can get an incorrect result or see strange exceptions that can be difficult to debug.\n",
    "\n",
    "# There are two things you can do with null values: you can explicitly drop nulls or you can fill them with a value (globally or on a per-column basis). Let’s experiment with each of these now.\n",
    "\n",
    "# Spark includes a function to allow you to select the first non-null value from a set of columns by using the coalesce function. In this case, there are no null values, so it simply returns the first column:\n",
    "    \n",
    "from pyspark.sql.functions import coalesce, col\n",
    "df.select(coalesce(col(\"Description\"), col(\"CustomerId\"))).show()\n",
    "\n",
    "# spark.sql(\"SELECT ifnull(null, 'return_value'), ifnull('ifnull_default', 'return_value'), nullif('value', 'value'), nvl(null, 'return_value'), nvl2('not_null', 'return_value', 'else_value') FROM dfTable LIMIT 1\").show()\n",
    "\n",
    "# ifnull allows you to select the second value if the first is null, and defaults to the first. \n",
    "spark.sql(\"SELECT ifnull(null, 'return_value'), ifnull('ifnull_default', 'return_value') FROM dfTable LIMIT 1\").show()\n",
    "\n",
    "# use nullif, which returns null if the two values are equal or else returns the second if they are not. \n",
    "spark.sql(\"SELECT nullif(null, null), nullif('value', 'value'), nullif(null, 'value'), nullif('notvalue', 'value') FROM dfTable LIMIT 1\").show()\n",
    "\n",
    "# nvl returns the second value if the first is null, but defaults to the first\n",
    "spark.sql(\"SELECT nvl(null, 'second_value'), nvl('first_value', 'second_value') FROM dfTable LIMIT 1\").show()\n",
    "\n",
    "# nvl2 returns the second value if the first is not null; otherwise, it will return the last specified value\n",
    "spark.sql(\"SELECT nvl2(null, 'second_value','second_value2'), nvl2('first_value', 'second_value','second_value2') FROM dfTable LIMIT 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b84ac5f9-264c-429e-b410-8def2b96af72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 5 rows\n\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 5 rows\n\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 5 rows\n\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Drop Null Values\n",
    "# The simplest function is drop, which removes rows that contain nulls. The default is to drop any row in which any value is null:\n",
    "df.na.drop().show(5)\n",
    "df.na.drop(\"any\").show(5)\n",
    "\n",
    "# Using “all” drops the row only if all values are null or NaN for that row:\n",
    "df.na.drop(\"all\").show(5)\n",
    "\n",
    "# We can also apply this to certain sets of columns by passing in an array of columns\n",
    "df.na.drop(\"all\", subset=[\"StockCode\", \"InvoiceNo\"]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab767c99-12d9-43f9-9e4a-69ecd35f900c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+---------+\n|RollNo| FName| LName|    Score|\n+------+------+------+---------+\n|  null|Uddhav|Savani|{P -> 20}|\n|     2|   Dev| Patel|{P -> 23}|\n|     1|  null|Savani|{M -> 21}|\n|     2|   Dev| Patel|{M -> 25}|\n|     1|Uddhav|Savani|{C -> 50}|\n|     2|   Dev| Patel|{C -> 80}|\n|     2|   Dev|  null|     null|\n+------+------+------+---------+\n\nroot\n |-- RollNo: long (nullable = true)\n |-- FName: string (nullable = true)\n |-- LName: string (nullable = true)\n |-- Score: map (nullable = true)\n |    |-- key: string\n |    |-- value: long (valueContainsNull = true)\n\n+------+------+------+---------+\n|RollNo| FName| LName|    Score|\n+------+------+------+---------+\n|  null|Uddhav|Savani|{P -> 20}|\n|     2|   Dev| Patel|{P -> 23}|\n|     1|  all2|Savani|{M -> 21}|\n|     2|   Dev| Patel|{M -> 25}|\n|     1|Uddhav|Savani|{C -> 50}|\n|     2|   Dev| Patel|{C -> 80}|\n|     2|   Dev|  all2|     null|\n+------+------+------+---------+\n\n+------+------+------+---------+\n|RollNo| FName| LName|    Score|\n+------+------+------+---------+\n|     5|Uddhav|Savani|{P -> 20}|\n|     2|   Dev| Patel|{P -> 23}|\n|     1|  null|Savani|{M -> 21}|\n|     2|   Dev| Patel|{M -> 25}|\n|     1|Uddhav|Savani|{C -> 50}|\n|     2|   Dev| Patel|{C -> 80}|\n|     2|   Dev|  null|     null|\n+------+------+------+---------+\n\nColumn<'19'>\n+------+-------+-------+---------+\n|RollNo|  FName|  LName|    Score|\n+------+-------+-------+---------+\n|     0| Uddhav| Savani|{P -> 20}|\n|     2|    Dev|  Patel|{P -> 23}|\n|     1|Unknown| Savani|{M -> 21}|\n|     2|    Dev|  Patel|{M -> 25}|\n|     1| Uddhav| Savani|{C -> 50}|\n|     2|    Dev|  Patel|{C -> 80}|\n|     2|    Dev|Unknown|     null|\n+------+-------+-------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# Fill null Values\n",
    "# you can fill one or more columns with a set of values. This can be done by specifying a map—that is a particular value and a set of columns.\n",
    "\n",
    "from pyspark.sql.types import ArrayType, StringType, LongType, StructType, StructField, MapType\n",
    "smapleListSchema = StructType([\n",
    "    StructField(\"RollNo\", LongType(), True),\n",
    "    StructField(\"FName\", StringType(), True),\n",
    "    StructField(\"LName\", StringType(), True),\n",
    "    StructField(\"Score\", MapType(StringType(), LongType()), True)\n",
    "])\n",
    "\n",
    "sampleList = [\n",
    "    (None, \"Uddhav\", \"Savani\", {\"P\" : 20}),\n",
    "    (2, \"Dev\", \"Patel\", {\"P\" :23}),\n",
    "    (1, None, \"Savani\", {\"M\" : 21}),\n",
    "    (2, \"Dev\", \"Patel\", {\"M\" :25}),\n",
    "    (1, \"Uddhav\", \"Savani\", {\"C\" : 50}),\n",
    "    (2, \"Dev\", \"Patel\", {\"C\" :80}),\n",
    "    (2, \"Dev\", None, None)\n",
    "]\n",
    "sampleDF = spark.createDataFrame(data=sampleList, schema=smapleListSchema)\n",
    "sampleDF.show()\n",
    "sampleDF.printSchema()\n",
    "sampleDF.na.fill(\"all2\").show()\n",
    "sampleDF.na.fill(5).show()\n",
    "\n",
    "# sampleDF.select(create_map(lit(\"P\"), lit(10))).show()\n",
    "print (lit(19))\n",
    "from pyspark.sql.functions import create_map, lit\n",
    "# sampleDF.na.fill(\n",
    "#     {\n",
    "#         \"RollNo\": 0, \n",
    "#         \"FName\": \"Unknown\", \n",
    "#         \"LName\": \"Unknown\",\n",
    "#         \"Score\": create_map([lit(\"P\"),lit(10)]) \n",
    "#     }\n",
    "# ).show()\n",
    "\n",
    "# DataFrame.fillna(value, subset=None)[source]\n",
    "# Parameters\n",
    "#     valueint, float, string, bool or dict\n",
    "\n",
    "# This Is Not Supported As Per API Docs.  Value to replace null values with. If the value is a dict, then subset is ignored and value must be a mapping from column name (string) to replacement value. The replacement value must be an int, float, boolean, or string.\n",
    "\n",
    "sampleDF.na.fill(\n",
    "    {\n",
    "        \"RollNo\": 0, \n",
    "        \"FName\": \"Unknown\", \n",
    "        \"LName\": \"Unknown\"\n",
    "    }\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f03c54b-05be-4d58-a0e0-8ec6b6fc0551",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|Description                        |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |\n+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER |6       |2010-12-01 08:26:00|2.55     |17850.0   |United Kingdom|\n|536365   |71053    |WHITE METAL LANTERN                |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n|536365   |84406B   |CREAM CUPID HEARTS COAT HANGER     |8       |2010-12-01 08:26:00|2.75     |17850.0   |United Kingdom|\n|536365   |84029G   |KNITTED UNION FLAG HOT WATER BOTTLE|6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n|536365   |84029E   |RED WOOLLY HOTTIE WHITE HEART.     |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 5 rows\n\n+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|Description                        |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |\n+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n|536365   |85123A   |REPLACED                           |6       |2010-12-01 08:26:00|2.55     |17850.0   |United Kingdom|\n|536365   |71053    |WHITE METAL LANTERN                |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n|536365   |84406B   |REPLACED2                          |8       |2010-12-01 08:26:00|2.75     |17850.0   |United Kingdom|\n|536365   |84029G   |KNITTED UNION FLAG HOT WATER BOTTLE|6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n|536365   |84029E   |RED WOOLLY HOTTIE WHITE HEART.     |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# replace \n",
    "df.show(5,truncate=False)\n",
    "df.na.replace([\"WHITE HANGING HEART T-LIGHT HOLDER\", \"CREAM CUPID HEARTS COAT HANGER\"], [\"REPLACED\", \"REPLACED2\"], \"Description\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f4b07b6-7195-4028-97b2-1c3d87741343",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ordering\n",
    "# use asc_nulls_first, desc_nulls_first, asc_nulls_last, or desc_nulls_last to specify where you would like your null values to appear in an ordered DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3037a97-1cf7-4768-91f8-ecddee2a4a6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+\n|complex                                     |InvoiceNo|StockCode|Description                       |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |\n+--------------------------------------------+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+\n|{WHITE HANGING HEART T-LIGHT HOLDER, 536365}|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER|6       |2010-12-01 08:26:00|2.55     |17850.0   |United Kingdom|\n|{WHITE METAL LANTERN, 536365}               |536365   |71053    |WHITE METAL LANTERN               |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n+--------------------------------------------+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 2 rows\n\n+--------------------------------------------+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+\n|complex                                     |InvoiceNo|StockCode|Description                       |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |\n+--------------------------------------------+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+\n|{WHITE HANGING HEART T-LIGHT HOLDER, 536365}|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER|6       |2010-12-01 08:26:00|2.55     |17850.0   |United Kingdom|\n|{WHITE METAL LANTERN, 536365}               |536365   |71053    |WHITE METAL LANTERN               |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n+--------------------------------------------+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 2 rows\n\nroot\n |-- complex: struct (nullable = false)\n |    |-- Description: string (nullable = true)\n |    |-- InvoiceNo: string (nullable = true)\n |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- InvoiceDate: timestamp (nullable = true)\n |-- UnitPrice: double (nullable = true)\n |-- CustomerID: double (nullable = true)\n |-- Country: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Complex Types\n",
    "# Structs\n",
    "# We can create a struct by wrapping a set of columns in parenthesis in a query\n",
    "df.selectExpr(\"(Description, InvoiceNo) as complex\", \"*\").show(2, truncate=False)\n",
    "df.selectExpr(\"struct(Description, InvoiceNo) as complex\", \"*\").show(2, truncate=False)\n",
    "df.selectExpr(\"(Description, InvoiceNo) as complex\", \"*\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75d1bb6b-3674-467d-b128-e08449138afc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|         Description|\n+--------------------+\n|WHITE HANGING HEA...|\n| WHITE METAL LANTERN|\n|CREAM CUPID HEART...|\n|KNITTED UNION FLA...|\n|RED WOOLLY HOTTIE...|\n|SET 7 BABUSHKA NE...|\n|GLASS STAR FROSTE...|\n|HAND WARMER UNION...|\n|HAND WARMER RED P...|\n|ASSORTED COLOUR B...|\n|POPPY'S PLAYHOUSE...|\n|POPPY'S PLAYHOUSE...|\n|FELTCRAFT PRINCES...|\n|IVORY KNITTED MUG...|\n|BOX OF 6 ASSORTED...|\n|BOX OF VINTAGE JI...|\n|BOX OF VINTAGE AL...|\n|HOME BUILDING BLO...|\n|LOVE BUILDING BLO...|\n|RECIPE BOX WITH M...|\n+--------------------+\nonly showing top 20 rows\n\n+--------------------+\n| complex.Description|\n+--------------------+\n|WHITE HANGING HEA...|\n| WHITE METAL LANTERN|\n|CREAM CUPID HEART...|\n|KNITTED UNION FLA...|\n|RED WOOLLY HOTTIE...|\n|SET 7 BABUSHKA NE...|\n|GLASS STAR FROSTE...|\n|HAND WARMER UNION...|\n|HAND WARMER RED P...|\n|ASSORTED COLOUR B...|\n|POPPY'S PLAYHOUSE...|\n|POPPY'S PLAYHOUSE...|\n|FELTCRAFT PRINCES...|\n|IVORY KNITTED MUG...|\n|BOX OF 6 ASSORTED...|\n|BOX OF VINTAGE JI...|\n|BOX OF VINTAGE AL...|\n|HOME BUILDING BLO...|\n|LOVE BUILDING BLO...|\n|RECIPE BOX WITH M...|\n+--------------------+\nonly showing top 20 rows\n\n+----------------------------------+---------+\n|Description                       |InvoiceNo|\n+----------------------------------+---------+\n|WHITE HANGING HEART T-LIGHT HOLDER|536365   |\n|WHITE METAL LANTERN               |536365   |\n+----------------------------------+---------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import struct, col\n",
    "complexDF = df.select(struct(\"Description\", \"InvoiceNo\").alias(\"complex\"))\n",
    "complexDF.createOrReplaceTempView(\"complexDF\")\n",
    "\n",
    "# Use Dot Syntex Or GetField Function to access inner Field\n",
    "complexDF.select(\"complex.Description\").show()\n",
    "complexDF.select(col(\"complex\").getField(\"Description\")).show()\n",
    "\n",
    "# We can also query all values in the struct by using *. This brings up all the columns to the top-level DataFrame:\n",
    "complexDF.select(\"complex.*\").show(2, truncate=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4beb59d2-538b-4354-bfbf-207f90a8bc82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+\n|split(Description,  , -1)               |\n+----------------------------------------+\n|[WHITE, HANGING, HEART, T-LIGHT, HOLDER]|\n|[WHITE, METAL, LANTERN]                 |\n+----------------------------------------+\nonly showing top 2 rows\n\nroot\n |-- split(Description,  , -1): array (nullable = true)\n |    |-- element: string (containsNull = false)\n\n+--------------------+\n|description_split[0]|\n+--------------------+\n|WHITE               |\n|WHITE               |\n+--------------------+\nonly showing top 2 rows\n\n+-------------------------------+\n|size(split(Description,  , -1))|\n+-------------------------------+\n|                              5|\n|                              3|\n+-------------------------------+\nonly showing top 2 rows\n\n+-----------------------------------------------+\n|array_contains(split(Description,  , -1), WHIT)|\n+-----------------------------------------------+\n|                                          false|\n|                                          false|\n+-----------------------------------------------+\nonly showing top 2 rows\n\n+------------------------------------------------+\n|array_contains(split(Description,  , -1), WHITE)|\n+------------------------------------------------+\n|                                            true|\n|                                            true|\n+------------------------------------------------+\nonly showing top 2 rows\n\n+-------+\n|    col|\n+-------+\n|  WHITE|\n|HANGING|\n+-------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Arrays As Complex Data Types\n",
    "# 1) Split\n",
    "from pyspark.sql.functions import split, col\n",
    "df.select(split(df.Description, \" \")).show(2, truncate=False)\n",
    "df.select(split(df.Description, \" \")).printSchema()\n",
    "\n",
    "# Array Element Level Access\n",
    "df.withColumn(\"description_split\", split(col(\"Description\"), \" \")).selectExpr(\"description_split[0]\").show(2, truncate=False)\n",
    "\n",
    "# Array Length\n",
    "from pyspark.sql.functions import size\n",
    "df.select(size(split(col(\"Description\"), \" \"))).show(2) # shows 5 and 3\n",
    "\n",
    "# array_contains\n",
    "from pyspark.sql.functions import array_contains\n",
    "df.select(array_contains(split(col(\"Description\"), \" \"), \"WHIT\")).show(2) \n",
    "df.select(array_contains(split(col(\"Description\"), \" \"), \"WHITE\")).show(2) \n",
    "\n",
    "# explode : It Crates One Row For Each Split in the feild in below example\n",
    "from pyspark.sql.functions import explode\n",
    "df.select(explode(split(col(\"Description\"), \" \"))).show(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6d6f3c2-3b0b-442c-9613-1825774abc42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n|map(P, 10)|\n+----------+\n| {P -> 10}|\n| {P -> 10}|\n| {P -> 10}|\n| {P -> 10}|\n| {P -> 10}|\n+----------+\nonly showing top 5 rows\n\n+--------------------+\n|         complex_map|\n+--------------------+\n|{WHITE HANGING HE...|\n|{WHITE METAL LANT...|\n+--------------------+\nonly showing top 2 rows\n\n+--------------+\n|complex_map[P]|\n+--------------+\n|            10|\n|            10|\n+--------------+\nonly showing top 2 rows\n\n+--------------------+------+\n|                 key| value|\n+--------------------+------+\n|WHITE HANGING HEA...|536365|\n| WHITE METAL LANTERN|536365|\n+--------------------+------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Maps\n",
    "\n",
    "from pyspark.sql.functions import lit, create_map\n",
    "df.select(create_map(lit(\"P\"), lit(10))).show(5)\n",
    "\n",
    "df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\\\n",
    ".show(2)\n",
    "\n",
    "# You can query them by using the proper key. A missing key returns null:\n",
    "df.select(create_map(lit(\"P\"), lit(10)).alias('complex_map'))\\\n",
    ".selectExpr('complex_map[\"P\"]').show(2)\n",
    "\n",
    "# explode Map, 1 Key Of Map Will Create One New Line For each Row\n",
    "df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\\\n",
    ".selectExpr(\"explode(complex_map)\").show(2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbcb13f7-4faa-4792-a1b4-250074c03ec7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------+\n|column|c0                     |\n+------+-----------------------+\n|2     |{\"myJSONValue\":[1,2,3]}|\n+------+-----------------------+\n\nroot\n |-- column: string (nullable = true)\n |-- c0: string (nullable = true)\n\n+--------------------------------------------------------------------------+\n|to_json(myStruct)                                                         |\n+--------------------------------------------------------------------------+\n|{\"InvoiceNo\":\"536365\",\"Description\":\"WHITE HANGING HEART T-LIGHT HOLDER\"} |\n|{\"InvoiceNo\":\"536365\",\"Description\":\"WHITE METAL LANTERN\"}                |\n|{\"InvoiceNo\":\"536365\",\"Description\":\"CREAM CUPID HEARTS COAT HANGER\"}     |\n|{\"InvoiceNo\":\"536365\",\"Description\":\"KNITTED UNION FLAG HOT WATER BOTTLE\"}|\n|{\"InvoiceNo\":\"536365\",\"Description\":\"RED WOOLLY HOTTIE WHITE HEART.\"}     |\n+--------------------------------------------------------------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Working with JSON\n",
    "jsonDF = spark.range(1).selectExpr(\"\"\"\n",
    "'{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}' as jsonString\"\"\")\n",
    "\n",
    "from pyspark.sql.functions import get_json_object, json_tuple\n",
    "jsonDF.select(\n",
    "get_json_object(col(\"jsonString\"), \"$.myJSONKey.myJSONValue[1]\").alias(\"column\"),\n",
    "json_tuple(col(\"jsonString\"), \"myJSONKey\")).show(2, truncate=False)\n",
    "\n",
    "jsonDF.select(\n",
    "get_json_object(col(\"jsonString\"), \"$.myJSONKey.myJSONValue[1]\").alias(\"column\"),\n",
    "json_tuple(col(\"jsonString\"), \"myJSONKey\")).printSchema()\n",
    "\n",
    "\n",
    "# We can also  turn a StructType into a JSON string by using the to_json function\n",
    "from pyspark.sql.functions import to_json\n",
    "df.selectExpr(\"(InvoiceNo, Description) as myStruct\")\\\n",
    ".select(to_json(col(\"myStruct\"))).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d2e8f30-eee9-45e5-8c3f-dc48711b92e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n|  from_json(newJSON)|             newJSON|\n+--------------------+--------------------+\n|{536365, WHITE HA...|{\"InvoiceNo\":\"536...|\n|{536365, WHITE ME...|{\"InvoiceNo\":\"536...|\n+--------------------+--------------------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "# You can use the from_json function to parse this (or other JSON data) back in. This naturally requires you to specify a schema, and optionally you can specify a map of options, as well:\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import *\n",
    "parseSchema = StructType((\n",
    "StructField(\"InvoiceNo\",StringType(),True),\n",
    "StructField(\"Description\",StringType(),True)))\n",
    "df.selectExpr(\"(InvoiceNo, Description) as myStruct\")\\\n",
    ".select(to_json(col(\"myStruct\")).alias(\"newJSON\"))\\\n",
    ".select(from_json(col(\"newJSON\"), parseSchema), col(\"newJSON\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b56199b7-79b4-471c-8e8b-d50c896670c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# UDFs\n",
    "# By default, these functions are registered as temporary functions to be used in that specific SparkSession or Context.\n",
    "# When you use the function, there are essentially two different things that occur. If the function is written in Scala or Java, you can use it within the Java Virtual Machine (JVM). This means that there will be little performance penalty aside from the fact that you can’t take advantage of code generation capabilities that Spark has for built-in functions. There can be performance issues if you create or use a lot of objects; we cover that in the section on optimization in Chapter 19.\n",
    "\n",
    "# If the function is written in Python, something quite different happens. Spark starts a Python process on the worker, serializes all of the data to a format that Python can understand (remember, it was in the JVM earlier), executes the function row by row on that data in the Python process, and then finally returns the results of the row operations to the JVM and Spark\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba6b218d-2a04-4ff5-a23b-afa1f74a95e1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Starting this Python process is expensive, but the real cost is in serializing the data to Python. This is\n",
    "costly for two reasons: it is an expensive computation, but also, after the data enters Python, Spark\n",
    "cannot manage the memory of the worker. This means that you could potentially cause a worker to fail\n",
    "if it becomes resource constrained (because both the JVM and Python are competing for memory on\n",
    "the same machine). We recommend that you write your UDFs in Scala or Java—the small amount of\n",
    "time it should take you to write the function in Scala will always yield significant speed ups, and on\n",
    "top of that, you can still use the function from Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "304302eb-fce9-41b7-8fc5-84523ba412a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|power3(num)|\n+-----------+\n|          0|\n|          1|\n+-----------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "udfExampleDF = spark.range(5).toDF(\"num\")\n",
    "\n",
    "def power3(double_value):\n",
    "    return double_value ** 3\n",
    "\n",
    "power3(2.0) \n",
    "\n",
    "# First, we need to register the function to make it available as a DataFrame function:\n",
    "from pyspark.sql.functions import udf\n",
    "power3udf = udf(power3)\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "udfExampleDF.select(power3udf(col(\"num\"))).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "873e6f49-d1fe-430a-8953-6d0aca25778e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "While using UDF. It is important to note that specifying the return type is not necessary, but it is a best practice.\n",
    "\n",
    "If you specify the type that doesn’t align with the actual type returned by the function, Spark will\n",
    "not throw an error but will just return null to designate a failure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "962d9204-861b-40eb-8885-96dbe73f7666",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n|power3py(num)|\n+-------------+\n|         null|\n|         null|\n+-------------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "spark.udf.register(\"power3py\", power3, DoubleType())\n",
    "\n",
    "udfExampleDF.selectExpr(\"power3py(num)\").show(2)\n",
    "\n",
    "# This is because the range creates integers. When integers are operated on in Python, Python won’t convert them into floats (the corresponding type to Spark’s double type), therefore we see null. We can remedy this by ensuring that our Python function returns a float instead of an integer and the function will behave correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9c0c9bc-3472-4e18-ab07-320639ec6108",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As a last note, you can also use UDF/UDAF creation via a Hive syntax. To allow for this, first\n",
    "you must enable Hive support when they create their SparkSession (via\n",
    "SparkSession.builder().enableHiveSupport()). Then you can register UDFs in SQL. This\n",
    "is only supported with precompiled Scala and Java packages, so you’ll need to specify them as a\n",
    "dependency:\n",
    "\n",
    "CREATE TEMPORARY FUNCTION myFunc AS 'com.organization.hive.udf.FunctionName'\n",
    "\n",
    "Additionally, you can register this as a permanent function in the Hive Metastore by removing\n",
    "TEMPORARY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50070834-f604-4bd5-b9ce-fddec5f5fdf3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1174254120656184,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark The Definative Guide Ch 6",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
