{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fc504a2-7bf9-4859-aabe-faa6993f8af2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[1]: True"
     ]
    }
   ],
   "source": [
    "dbutils.fs.cp(\n",
    "  'dbfs:/FileStore/Spark_The_Definitive_Guide_master.zip', \n",
    "  'file:/tmp/Spark_The_Definitive_Guide_master.zip'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "867d2098-e7a4-4fa7-bf4e-89a10ca5be32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh unzip /tmp/Spark_The_Definitive_Guide_master.zip -d /tmp > /tmp/unzipout 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65935166-573a-4be4-8401-601ed9317516",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Spark also allows us to create the following groupings types:\n",
    "1) The simplest grouping is to just summarize a complete DataFrame by performing an aggregation in a select statement.\n",
    "2) A “group by” allows you to specify one or more keys as well as one or more aggregation functions to transform the value columns.\n",
    "3) A “window” gives you the ability to specify one or more keys as well as one or more aggregation functions to transform the value columns. However, the rows input to the function are somehow related to the current row.\n",
    "4) A “grouping set,” which you can use to aggregate at multiple different levels. Grouping sets are available as a primitive in SQL and via rollups and cubes in DataFrames.\n",
    "5) A “rollup” makes it possible for you to specify one or more keys as well as one or more aggregation functions to transform the value columns, which will be summarized hierarchically.\n",
    "6) A “cube” allows you to specify one or more keys as well as one or more aggregation functions to transform the value columns, which will be summarized across all combinations of columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d89ab374-cfa9-4325-bd8c-9c9f6188fe45",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Each grouping returns a RelationalGroupedDataset on which we specify our aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1c857b1-243c-419d-adea-18415966bdd6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------------------------------+--------+--------------+---------+----------+--------------+\n|InvoiceNo|StockCode|Description                        |Quantity|InvoiceDate   |UnitPrice|CustomerID|Country       |\n+---------+---------+-----------------------------------+--------+--------------+---------+----------+--------------+\n|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER |6       |12/1/2010 8:26|2.55     |17850     |United Kingdom|\n|536365   |71053    |WHITE METAL LANTERN                |6       |12/1/2010 8:26|3.39     |17850     |United Kingdom|\n|536365   |84406B   |CREAM CUPID HEARTS COAT HANGER     |8       |12/1/2010 8:26|2.75     |17850     |United Kingdom|\n|536365   |84029G   |KNITTED UNION FLAG HOT WATER BOTTLE|6       |12/1/2010 8:26|3.39     |17850     |United Kingdom|\n|536365   |84029E   |RED WOOLLY HOTTIE WHITE HEART.     |6       |12/1/2010 8:26|3.39     |17850     |United Kingdom|\n+---------+---------+-----------------------------------+--------+--------------+---------+----------+--------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(\"file:///tmp/Spark-The-Definitive-Guide-master/data/retail-data/all/*.csv\")\\\n",
    ".coalesce(5)\n",
    "df.cache()\n",
    "df.createOrReplaceTempView(\"dfTable\")\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b3245a4-6535-4902-b31d-d1a0059d7698",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The Basic Aggrigation Applies To Entire Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b09ad24-5777-4b3d-b8a6-dbd6b8c6afbb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "541909\n"
     ]
    }
   ],
   "source": [
    "print (df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3261fca-c713-4770-8d7a-54a995d75940",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Most Of The Aggrigation function is available in\n",
    "pyspark.sql.functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81dd056f-461c-4149-b97c-b421be51a619",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Count:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbe847d1-1335-4691-961e-3a480fe61f8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n|count(Quantity)|\n+---------------+\n|         541909|\n+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "df.select(count(df.Quantity)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d25929d0-8cad-49bc-be27-a371ad7e4cb1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "There are a number of gotchas when it comes to null values and counting. For instance, when performing a count(*), Spark will count null values (including rows containing all nulls). However, when counting an individual column, Spark will not count the null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2393ace-29bd-4015-a05e-d7e6e325d1c5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "countDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "936744ca-08b9-46f3-89e3-735674ab0461",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n|count(DISTINCT StockCode)|\n+-------------------------+\n|                     4070|\n+-------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "df.select(countDistinct(df.StockCode)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae86ef31-ecd0-4242-935a-11df175fc602",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "we find ourselves working with large datasets and the exact distinct count is irrelevant. There are times when an approximation to a certain degree of accuracy will work just fine, and for that, you can use the approx_count_distinct function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9530542-93c6-41e5-9d3b-ab311d72ad7f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n|approx_count_distinct(StockCode)|\n+--------------------------------+\n|                            3364|\n+--------------------------------+\n\n+--------------------------------+\n|approx_count_distinct(StockCode)|\n+--------------------------------+\n|                            2944|\n+--------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct\n",
    "df.select(approx_count_distinct(df.StockCode, 0.1)).show()\n",
    "df.select(approx_count_distinct(df.StockCode, 0.2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "336ccf4f-e652-407d-882d-80261db43cb9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "first and last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13cc720c-de15-4a5c-946a-2caf20ea0689",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+\n|first(StockCode)|last(StockCode)|\n+----------------+---------------+\n|          85123A|          22138|\n+----------------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first, last\n",
    "df.select(first(df.StockCode), last(df.StockCode)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a24a3e6-00cd-4e52-ab4b-1560175135b1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "min and max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fc9004e-a3c2-42cc-92a4-a8d6e00cd2b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n|min(UnitPrice)|max(UnitPrice)|\n+--------------+--------------+\n|     -11062.06|       38970.0|\n+--------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "df.select(min(df.UnitPrice), max(df.UnitPrice)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca555902-da05-481c-99fb-1f909935c649",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n|sum(Quantity)|\n+-------------+\n|      5176450|\n+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Sum\n",
    "from pyspark.sql.functions import sum\n",
    "df.select(sum(df.Quantity)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54c84db5-be95-4f6a-bc63-4836492c45f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/functions.py:723: FutureWarning: Deprecated in 3.2, use sum_distinct instead.\n  warnings.warn(\"Deprecated in 3.2, use sum_distinct instead.\", FutureWarning)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n|sum(DISTINCT Quantity)|\n+----------------------+\n|                 29310|\n+----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# sumDistinct\n",
    "from pyspark.sql.functions import sumDistinct\n",
    "df.select(sumDistinct(df.Quantity)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35a9b173-e356-4f4c-83c3-94f2ca79e111",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n|   avg(UnitPrice)|\n+-----------------+\n|4.611113626089214|\n+-----------------+\n\n+--------------------------------------+----------------+----------------+\n|(total_purchases / total_transactions)|   avg_purchases|  mean_purchases|\n+--------------------------------------+----------------+----------------+\n|                      9.55224954743324|9.55224954743324|9.55224954743324|\n+--------------------------------------+----------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "#avg\n",
    "from pyspark.sql.functions import avg, count, sum, expr\n",
    "df.select(avg(df.UnitPrice)).show()\n",
    "\n",
    "df.select(\n",
    "    count(\"Quantity\").alias(\"total_transactions\"),\n",
    "    sum(\"Quantity\").alias(\"total_purchases\"),\n",
    "    avg(\"Quantity\").alias(\"avg_purchases\"),\n",
    "    expr(\"mean(Quantity)\").alias(\"mean_purchases\")\n",
    ") \\\n",
    ".selectExpr(\n",
    "    \"total_purchases/total_transactions\",\n",
    "    \"avg_purchases\",\n",
    "    \"mean_purchases\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05f952d0-1333-477e-a648-7c4aec7d9aab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------------------+---------------------+\n| var_pop(Quantity)|var_samp(Quantity)|stddev_pop(Quantity)|stddev_samp(Quantity)|\n+------------------+------------------+--------------------+---------------------+\n|47559.303646609056|47559.391409298754|  218.08095663447796|   218.08115785023418|\n+------------------+------------------+--------------------+---------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Variance and Standard Deviation\n",
    "from pyspark.sql.functions import var_pop, stddev_pop\n",
    "from pyspark.sql.functions import var_samp, stddev_samp\n",
    "\n",
    "df.select(var_pop(\"Quantity\"), var_samp(\"Quantity\"),\n",
    "stddev_pop(\"Quantity\"), stddev_samp(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56288607-90a5-4677-874b-6e6107af8232",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Skewness and kurtosis are both measurements of extreme points in your data. Skewness measures the asymmetry of the values in your data around the mean, whereas kurtosis is a measure of the tail of data. These are both relevant specifically when modeling your data as a probability distribution of a random variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8eeafee5-e68d-4a43-aafd-303b1f7b6937",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n| skewness(Quantity)|kurtosis(Quantity)|\n+-------------------+------------------+\n|-0.2640755761052562|119768.05495536952|\n+-------------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# skewness and kurtosis\n",
    "from pyspark.sql.functions import skewness, kurtosis\n",
    "df.select(skewness(\"Quantity\"), kurtosis(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b8df440-c673-4a1a-ae68-c7c9575af59b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We discussed single column aggregations, but some functions compare the interactions of the values in two difference columns together. Two of these functions are cov and corr, for covariance and correlation, respectively. Correlation measures the Pearson correlation coefficient, which is scaled between –1 and +1. The covariance is scaled according to the inputs in the data.\n",
    "\n",
    "Like the var function, covariance can be calculated either as the sample covariance or the population covariance. Therefore it can be important to specify which formula you want to use. Correlation has no notion of this and therefore does not have calculations for population or\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d19b5d83-508f-401d-8c00-4db113db0ddc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------------------------------+------------------------------+\n|corr(InvoiceNo, Quantity)|covar_samp(InvoiceNo, Quantity)|covar_pop(InvoiceNo, Quantity)|\n+-------------------------+-------------------------------+------------------------------+\n|     4.912186085635685E-4|             1052.7280543902734|            1052.7260778741693|\n+-------------------------+-------------------------------+------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr, covar_pop, covar_samp\n",
    "df.select(corr(\"InvoiceNo\", \"Quantity\"), covar_samp(\"InvoiceNo\", \"Quantity\"),\n",
    "covar_pop(\"InvoiceNo\", \"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "beb70551-22fa-488a-a883-fad67108beab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Aggregating to Complex Types<br>\n",
    "We can collect list of values of set of unique values on column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1dc691c-769b-49df-bcb7-dd93af23cff1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n|collect_set(Country)|collect_list(Country)|\n+--------------------+---------------------+\n|[Portugal, Italy,...| [United Kingdom, ...|\n+--------------------+---------------------+\n\nroot\n |-- collect_set(Country): array (nullable = false)\n |    |-- element: string (containsNull = false)\n |-- collect_list(Country): array (nullable = false)\n |    |-- element: string (containsNull = false)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set, collect_list\n",
    "df.agg(collect_set(\"Country\"), collect_list(\"Country\")).show()\n",
    "df.agg(collect_set(\"Country\"), collect_list(\"Country\")).printSchema()\n",
    "# Point To Note: Collect Set Also returns Array not Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75a40376-792d-43d0-bd1a-26bcb925ed6b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Grouping Operations<br>\n",
    "\n",
    "Grouping Can be done on one or more columns<br>\n",
    "Grouping is Lazily Evaluating Functions Which returns RelationalGroupedDataset<br>\n",
    "On the grouped Dataset we can further perform aggrigation which will return Dataframe, Defined aggrigations are also lazily eveluating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0f50505-77a1-4fb2-ab5b-34c5085ccd77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+\n|InvoiceNo|CustomerId|count|\n+---------+----------+-----+\n|   536846|     14573|   76|\n|   537026|     12395|   12|\n|   537883|     14437|    5|\n|   538068|     17978|   12|\n|   538279|     14952|    7|\n|   538800|     16458|   10|\n|   538942|     17346|   12|\n|  C539947|     13854|    1|\n|   540096|     13253|   16|\n|   540530|     14755|   27|\n|   541225|     14099|   19|\n|   541978|     13551|    4|\n|   542093|     17677|   16|\n|   536596|      null|    6|\n|   537252|      null|    1|\n|   538041|      null|    1|\n|   537159|     14527|   28|\n|   537213|     12748|    6|\n|   538191|     15061|   16|\n|  C539301|     13496|    1|\n+---------+----------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\", \"CustomerId\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11fff1c5-e889-4608-9fde-f84e27aee520",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------------+\n|InvoiceNo|quan|count(Quantity)|\n+---------+----+---------------+\n|   536596|   6|              6|\n|   536938|  14|             14|\n|   537252|   1|              1|\n|   537691|  20|             20|\n|   538041|   1|              1|\n|   538184|  26|             26|\n|   538517|  53|             53|\n|   538879|  19|             19|\n|   539275|   6|              6|\n|   539630|  12|             12|\n|   540499|  24|             24|\n|   540540|  22|             22|\n|  C540850|   1|              1|\n|   540976|  48|             48|\n|   541432|   4|              4|\n|   541518| 101|            101|\n|   541783|  35|             35|\n|   542026|   9|              9|\n|   542375|   6|              6|\n|   536597|  28|             28|\n+---------+----+---------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Grouping with Expression\n",
    "from pyspark.sql.functions import count\n",
    "df.groupBy(\"InvoiceNo\").agg(\n",
    "  count(\"Quantity\").alias(\"quan\"),\n",
    "  expr(\"count(Quantity)\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34a0b4ec-8b88-486f-b268-a1022d75f430",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+--------------------+\n|InvoiceNo|     avg(Quantity)|stddev_pop(Quantity)|\n+---------+------------------+--------------------+\n|   536596|               1.5|  1.1180339887498947|\n|   536938|33.142857142857146|  20.698023172885524|\n|   537252|              31.0|                 0.0|\n|   537691|              8.15|   5.597097462078001|\n|   538041|              30.0|                 0.0|\n|   538184|12.076923076923077|   8.142590198943392|\n|   538517|3.0377358490566038|  2.3946659604837897|\n|   538879|21.157894736842106|  11.811070444356483|\n|   539275|              26.0|  12.806248474865697|\n|   539630|20.333333333333332|  10.225241100118645|\n|   540499|              3.75|  2.6653642652865788|\n|   540540|2.1363636363636362|  1.0572457590557278|\n|  C540850|              -1.0|                 0.0|\n|   540976|10.520833333333334|   6.496760677872902|\n|   541432|             12.25|  10.825317547305483|\n|   541518| 23.10891089108911|  20.550782784878713|\n|   541783|11.314285714285715|   8.467657556242811|\n|   542026| 7.666666666666667|   4.853406592853679|\n|   542375|               8.0|  3.4641016151377544|\n|   536597|2.5357142857142856|  2.7448932175059566|\n+---------+------------------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\").agg(expr(\"avg(Quantity)\"),expr(\"stddev_pop(Quantity)\"))\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4215fa20-5c37-478e-98b2-fb794c484849",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Window Functions<br><br>\n",
    "\n",
    "Window functions are user to carry out unique aggrigation on specific Window Of Data which we define by reference to the current data. This window specification determines which rows will be passed in to this function. <br>\n",
    "\n",
    "The Difference Between Group By and Window Function\n",
    "=> A group-by takes data and every row can go only into one grouping where else a window function calculates return value for every input row of a table based on a group of rows called a frame. Each Row can fall into one or more then one row. For Example In case of rolling average each one row will end up in seven different frames.\n",
    "\n",
    "Spark supports three kinds of Window Function\n",
    "1) Ranking Functions\n",
    "2) Analytics Functions\n",
    "3) Aggregate Functions\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cd5b627-030b-4792-bede-d85941aa102c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- InvoiceDate: string (nullable = true)\n |-- UnitPrice: double (nullable = true)\n |-- CustomerID: integer (nullable = true)\n |-- Country: string (nullable = true)\n\n+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6| 2010-12-01|     2.55|     17850|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6| 2010-12-01|     3.39|     17850|United Kingdom|\n|   536365|   84406B|CREAM CUPID HEART...|       8| 2010-12-01|     2.75|     17850|United Kingdom|\n|   536365|   84029G|KNITTED UNION FLA...|       6| 2010-12-01|     3.39|     17850|United Kingdom|\n|   536365|   84029E|RED WOOLLY HOTTIE...|       6| 2010-12-01|     3.39|     17850|United Kingdom|\n|   536365|    22752|SET 7 BABUSHKA NE...|       2| 2010-12-01|     7.65|     17850|United Kingdom|\n|   536365|    21730|GLASS STAR FROSTE...|       6| 2010-12-01|     4.25|     17850|United Kingdom|\n|   536366|    22633|HAND WARMER UNION...|       6| 2010-12-01|     1.85|     17850|United Kingdom|\n|   536366|    22632|HAND WARMER RED P...|       6| 2010-12-01|     1.85|     17850|United Kingdom|\n|   536367|    84879|ASSORTED COLOUR B...|      32| 2010-12-01|     1.69|     13047|United Kingdom|\n|   536367|    22745|POPPY'S PLAYHOUSE...|       6| 2010-12-01|      2.1|     13047|United Kingdom|\n|   536367|    22748|POPPY'S PLAYHOUSE...|       6| 2010-12-01|      2.1|     13047|United Kingdom|\n|   536367|    22749|FELTCRAFT PRINCES...|       8| 2010-12-01|     3.75|     13047|United Kingdom|\n|   536367|    22310|IVORY KNITTED MUG...|       6| 2010-12-01|     1.65|     13047|United Kingdom|\n|   536367|    84969|BOX OF 6 ASSORTED...|       6| 2010-12-01|     4.25|     13047|United Kingdom|\n|   536367|    22623|BOX OF VINTAGE JI...|       3| 2010-12-01|     4.95|     13047|United Kingdom|\n|   536367|    22622|BOX OF VINTAGE AL...|       2| 2010-12-01|     9.95|     13047|United Kingdom|\n|   536367|    21754|HOME BUILDING BLO...|       3| 2010-12-01|     5.95|     13047|United Kingdom|\n|   536367|    21755|LOVE BUILDING BLO...|       3| 2010-12-01|     5.95|     13047|United Kingdom|\n|   536367|    21777|RECIPE BOX WITH M...|       4| 2010-12-01|     7.95|     13047|United Kingdom|\n+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\nonly showing top 20 rows\n\nroot\n |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- InvoiceDate: date (nullable = true)\n |-- UnitPrice: double (nullable = true)\n |-- CustomerID: integer (nullable = true)\n |-- Country: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "df.printSchema()\n",
    "dfWithDate = df.withColumn(\"InvoiceDate\", to_date(df[\"InvoiceDate\"], \"MM/d/yyyy H:mm\"))\n",
    "dfWithDate.show()\n",
    "dfWithDate.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8d6c54a-b3b0-4c6b-a226-77da6b3375a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<'max(Quantity) OVER (PARTITION BY CustomerID, InvoiceDate ORDER BY Quantity DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)'>\n+---------+---------+--------------------+--------+-----------+---------+----------+-------+-------------------+\n|InvoiceNo|StockCode|         Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|maxPurchaseQuantity|\n+---------+---------+--------------------+--------+-----------+---------+----------+-------+-------------------+\n|   562032|   84558A|3D DOG PICTURE PL...|      36| 2011-08-02|     2.95|     12347|Iceland|                 36|\n|   562032|    23308|SET OF 60 VINTAGE...|      24| 2011-08-02|     0.55|     12347|Iceland|                 36|\n|   562032|    84992|72 SWEETHEART FAI...|      24| 2011-08-02|     0.55|     12347|Iceland|                 36|\n|   562032|    84991|60 TEATIME FAIRY ...|      24| 2011-08-02|     0.55|     12347|Iceland|                 36|\n|   562032|    21975|PACK OF 60 DINOSA...|      24| 2011-08-02|     0.55|     12347|Iceland|                 36|\n|   562032|    21791|VINTAGE HEADS AND...|      24| 2011-08-02|     1.25|     12347|Iceland|                 36|\n|   562032|    23297|SET 40 HEART SHAP...|      16| 2011-08-02|     1.65|     12347|Iceland|                 36|\n|   562032|    23147|SINGLE ANTIQUE RO...|      12| 2011-08-02|     1.45|     12347|Iceland|                 36|\n|   562032|    22992|REVOLVER WOODEN R...|      12| 2011-08-02|     1.95|     12347|Iceland|                 36|\n|   562032|    23316|RED REFECTORY CLOCK |      12| 2011-08-02|     8.15|     12347|Iceland|                 36|\n|   562032|    20719|WOODLAND CHARLOTT...|      10| 2011-08-02|     0.85|     12347|Iceland|                 36|\n|   562032|   47559B| TEA TIME OVEN GLOVE|      10| 2011-08-02|     1.25|     12347|Iceland|                 36|\n|   562032|    22371|AIRLINE BAG VINTA...|       8| 2011-08-02|     4.25|     12347|Iceland|                 36|\n|   562032|    22727|ALARM CLOCK BAKEL...|       8| 2011-08-02|     3.75|     12347|Iceland|                 36|\n|   562032|    21578|WOODLAND DESIGN  ...|       6| 2011-08-02|     2.25|     12347|Iceland|                 36|\n|   562032|    22375|AIRLINE BAG VINTA...|       4| 2011-08-02|     4.25|     12347|Iceland|                 36|\n|   562032|    22374|AIRLINE BAG VINTA...|       4| 2011-08-02|     4.25|     12347|Iceland|                 36|\n|   562032|    22376|AIRLINE BAG VINTA...|       4| 2011-08-02|     4.25|     12347|Iceland|                 36|\n|   562032|    22372|AIRLINE BAG VINTA...|       4| 2011-08-02|     4.25|     12347|Iceland|                 36|\n|   562032|    23146|TRIPLE HOOK ANTIQ...|       4| 2011-08-02|     3.29|     12347|Iceland|                 36|\n+---------+---------+--------------------+--------+-----------+---------+----------+-------+-------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc, col, max\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "dateWindow = Window \\\n",
    "    .partitionBy(\"CustomerID\", \"InvoiceDate\")\\\n",
    "    .orderBy(desc(\"Quantity\"))\\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "maxPurchaseQuantity = max(col(\"Quantity\")).over(dateWindow)\n",
    "print(maxPurchaseQuantity)\n",
    "# print (type(maxPurchaseQuantity))\n",
    "dfWithDate.withColumn(\"maxPurchaseQuantity\", maxPurchaseQuantity).where(dfWithDate.CustomerID.isNotNull()).show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3174710710993915,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark_The_Definative_Guide_Ch_7",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
